# Swin Transformer + Focal Loss å®Œæ•´æ•™ç¨‹

## çš®è‚¤ç—…å˜åˆ†ç±»çš„æ·±åº¦å­¦ä¹ å®æˆ˜æŒ‡å—

> **é¡¹ç›®æ¦‚è¿°**: åŸºäºSwin Transformerå’ŒFocal Lossçš„åŒ»å­¦å›¾åƒåˆ†ç±»ç³»ç»Ÿ
> **åº”ç”¨åœºæ™¯**: çš®è‚¤ç—…å˜7åˆ†ç±»ï¼ˆé»‘è‰²ç´ ç˜¤æ£€æµ‹ï¼‰
> **æ ¸å¿ƒåˆ›æ–°**: å±‚æ¬¡åŒ–ç‰¹å¾æå– + è‡ªé€‚åº”ç±»åˆ«ä¸å¹³è¡¡å¤„ç† + åŒåˆ†æ”¯å¤šä»»åŠ¡å­¦ä¹ 
> **æ€§èƒ½æŒ‡æ ‡**: BCN20000 93.33% | HAM10000 ğŸ† 98.90% | MEL F1 0.977 | å¹³å‡ 96.12%

---

## ç›®å½•

### ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç†è®º
- [1. æ ¸å¿ƒæ¦‚å¿µ](#1-æ ¸å¿ƒæ¦‚å¿µ)
- [2. é—®é¢˜å®šä¹‰ä¸æŒ‘æˆ˜](#2-é—®é¢˜å®šä¹‰ä¸æŒ‘æˆ˜)
- [3. è§£å†³æ–¹æ¡ˆæ¶æ„](#3-è§£å†³æ–¹æ¡ˆæ¶æ„)

### ç¬¬äºŒéƒ¨åˆ†ï¼šç®—æ³•è¯¦è§£
- [4. Swin TransformeråŸç†](#4-swin-transformeråŸç†)
- [5. Focal Lossæœºåˆ¶](#5-focal-lossæœºåˆ¶)
- [6. åŒåˆ†æ”¯æ¶æ„è®¾è®¡](#6-åŒåˆ†æ”¯æ¶æ„è®¾è®¡)

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´è®­ç»ƒæµç¨‹
- [7. æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†](#7-æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†)
- [8. æ¨¡å‹è®¾è®¡ä¸å®ç°](#8-æ¨¡å‹è®¾è®¡ä¸å®ç°)
- [9. è®­ç»ƒé…ç½®ä¸ä¼˜åŒ–](#9-è®­ç»ƒé…ç½®ä¸ä¼˜åŒ–)
- [10. è®­ç»ƒå¾ªç¯ä¸ç›‘æ§](#10-è®­ç»ƒå¾ªç¯ä¸ç›‘æ§)
- [11. æ¨¡å‹è¯„ä¼°ä¸åˆ†æ](#11-æ¨¡å‹è¯„ä¼°ä¸åˆ†æ)

### ç¬¬å››éƒ¨åˆ†ï¼šå¯¹æ¯”ä¸å®æˆ˜
- [12. ä¸SOTAæ–¹æ³•å¯¹æ¯”](#12-ä¸sotaæ–¹æ³•å¯¹æ¯”)
- [13. å¼€æºé¡¹ç›®å¯¹æ¯”](#13-å¼€æºé¡¹ç›®å¯¹æ¯”)
- [14. å®æˆ˜æŠ€å·§ä¸ä¼˜åŒ–](#14-å®æˆ˜æŠ€å·§ä¸ä¼˜åŒ–)

### ç¬¬äº”éƒ¨åˆ†ï¼šå‚è€ƒèµ„æº
- [15. å‚è€ƒæ–‡çŒ®](#15-å‚è€ƒæ–‡çŒ®)
- [16. ä»£ç ä¸æ•°æ®](#16-ä»£ç ä¸æ•°æ®)

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºç¡€ç†è®º

## 1. æ ¸å¿ƒæ¦‚å¿µ

### 1.1 è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ

åœ¨åŒ»å­¦å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬é¢ä¸´**ä¸‰å¤§æ ¸å¿ƒæŒ‘æˆ˜**ï¼š

#### æŒ‘æˆ˜1: æåº¦ç±»åˆ«ä¸å¹³è¡¡

```python
# BCN20000æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ
class_distribution = {
    'NV (è‰¯æ€§ç—£)': '12,875æ ·æœ¬ (66.3%)',      # æœ€å¤š
    'MEL (é»‘è‰²ç´ ç˜¤)': '3,323æ ·æœ¬ (17.1%)',    # å…³é”®ç–¾ç—…
    'BKL (è„‚æº¢æ€§è§’åŒ–)': '2,624æ ·æœ¬ (13.5%)',
    'BCC (åŸºåº•ç»†èƒç™Œ)': '514æ ·æœ¬ (2.6%)',
    'AKIEC (å…‰åŒ–æ€§è§’åŒ–)': '67æ ·æœ¬ (0.3%)',
    'VASC (è¡€ç®¡ç—…å˜)': '15æ ·æœ¬ (0.1%)',
    'DF (çš®è‚¤çº¤ç»´ç˜¤)': '6æ ·æœ¬ (0.03%)'       # æœ€å°‘ï¼Œä¸å¹³è¡¡æ¯”ä¾‹ 2146:1
}

# ä¼ ç»Ÿäº¤å‰ç†µæŸå¤±çš„é—®é¢˜
traditional_ce_problem = """
æ¨¡å‹ä¼šåå‘å¤šæ•°ç±»(NV)ï¼Œå¿½ç•¥å°‘æ•°ç±»(DF)ï¼š
- NVå‡†ç¡®ç‡: 99% âœ“
- DFå‡†ç¡®ç‡: 0%  âœ— (å®Œå…¨å­¦ä¸åˆ°)
- æ•´ä½“å‡†ç¡®ç‡: 89% (è¢«å¤šæ•°ç±»ä¸»å¯¼ï¼Œå®é™…æ— ç”¨)
"""
```

#### æŒ‘æˆ˜2: å¤šå°ºåº¦ç‰¹å¾æå–

```python
# çš®è‚¤ç—…å˜çš„å¤šå°ºåº¦ç‰¹å¾
multi_scale_features = {
    'å±€éƒ¨çº¹ç†': 'ç»†å¾®çš„è¡¨é¢çº¹ç†ã€æ¯›å­”ã€çš®è‚¤çº¹ç†',
    'ä¸­å±‚å½¢çŠ¶': 'ç—…å˜è¾¹ç•Œã€å½¢çŠ¶è§„åˆ™æ€§ã€å¯¹ç§°æ€§',
    'é«˜å±‚è¯­ä¹‰': 'é¢œè‰²åˆ†å¸ƒã€æ•´ä½“æ¨¡å¼ã€ç—…å˜ç±»å‹',
    'å…¨å±€ä¸Šä¸‹æ–‡': 'ç—…å˜ä¸å‘¨å›´çš®è‚¤çš„å…³ç³»'
}

# ä¼ ç»Ÿæ–¹æ³•çš„å±€é™
limitations = {
    'CNN': 'æ„Ÿå—é‡æœ‰é™ï¼Œéš¾ä»¥æ•è·å…¨å±€ä¸Šä¸‹æ–‡',
    'ViT': 'è®¡ç®—å¤æ‚åº¦O(nÂ²)ï¼Œå¯¹ä¸­ç­‰è§„æ¨¡æ•°æ®é›†è¿‡æ‹Ÿåˆ'
}
```

#### æŒ‘æˆ˜3: å…³é”®ç–¾ç—…æ£€æµ‹

```python
# é»‘è‰²ç´ ç˜¤(MEL)çš„ç‰¹æ®Šæ€§
melanoma_importance = {
    'å±é™©æ€§': 'æœ€è‡´å‘½çš„çš®è‚¤ç™Œï¼Œ5å¹´ç”Ÿå­˜ç‡å–å†³äºæ—©æœŸå‘ç°',
    'æ··æ·†æ€§': 'ä¸è‰¯æ€§ç—£(NV)å¤–è§‚ç›¸ä¼¼ï¼Œå®¹æ˜“è¯¯è¯Š',
    'æ¼è¯Šä»£ä»·': 'æ¼è¯Šå¯èƒ½å±åŠç”Ÿå‘½',
    'æ£€æµ‹éœ€æ±‚': 'éœ€è¦ä¸“é—¨çš„åˆ¤åˆ«èƒ½åŠ›ï¼Œæé«˜æ•æ„Ÿåº¦'
}
```

### 1.2 æˆ‘ä»¬çš„è§£å†³æ–¹æ¡ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å®Œæ•´è§£å†³æ–¹æ¡ˆæ¶æ„                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æŒ‘æˆ˜1: ç±»åˆ«ä¸å¹³è¡¡
    â†“
è§£å†³æ–¹æ¡ˆ: Focal Loss
    - è‡ªé€‚åº”æƒé‡: (1-p_t)^Î³
    - æ˜“åˆ†æ ·æœ¬æƒé‡â†“99%
    - éš¾åˆ†æ ·æœ¬æƒé‡ä¿æŒ
    - æ€§èƒ½æå‡: +3.16%

æŒ‘æˆ˜2: å¤šå°ºåº¦ç‰¹å¾
    â†“
è§£å†³æ–¹æ¡ˆ: Swin Transformer
    - å±‚æ¬¡åŒ–æ¶æ„: 4ä¸ªstage
    - ç§»åŠ¨çª—å£æ³¨æ„åŠ›: O(n)å¤æ‚åº¦
    - å±€éƒ¨â†’å…¨å±€ç‰¹å¾
    - æ€§èƒ½æå‡: +0.91%

æŒ‘æˆ˜3: å…³é”®ç–¾ç—…æ£€æµ‹
    â†“
è§£å†³æ–¹æ¡ˆ: åŒåˆ†æ”¯æ¶æ„
    - é€šç”¨åˆ†æ”¯: 7åˆ†ç±»
    - MELä¸“é¡¹åˆ†æ”¯: 2åˆ†ç±»
    - æ³¨æ„åŠ›èåˆ: åŠ¨æ€æƒé‡
    - MEL F1æå‡: +4.6%

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
æœ€ç»ˆæ€§èƒ½ï¼ˆæœ€æ–°è¯„ä¼°ç»“æœï¼‰:
- BCN20000: 93.33% (vs ViT Baseline 89.81%, +3.52%)
- HAM10000: ğŸ† 98.90% (vs ViT Baseline 89.12%, +9.78%)
- MEL F1: 0.977 (HAM) / 0.974 (BCN)
- å¹³å‡å‡†ç¡®ç‡: 96.12% (æ‰€æœ‰æ¨¡å‹ä¸­æœ€é«˜)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 2. é—®é¢˜å®šä¹‰ä¸æŒ‘æˆ˜

### 2.1 ä¸šåŠ¡é—®é¢˜

**åŒ»å­¦è¯Šæ–­è¾…åŠ©ç³»ç»Ÿ**

```python
# é—®é¢˜ç±»å‹
task_definition = {
    'ä»»åŠ¡ç±»å‹': 'å¤šåˆ†ç±»é—®é¢˜ (Multi-class Classification)',
    'è¾“å…¥': 'çš®è‚¤é•œå›¾åƒ (224Ã—224 RGB)',
    'è¾“å‡º': '7ç§çš®è‚¤ç—…å˜ç±»åˆ«æ¦‚ç‡åˆ†å¸ƒ',
    'ç›®æ ‡': 'è¾…åŠ©åŒ»ç”Ÿè¯Šæ–­ï¼Œç‰¹åˆ«æ˜¯é»‘è‰²ç´ ç˜¤æ—©æœŸæ£€æµ‹'
}

# ä¸šåŠ¡ç›®æ ‡
business_goals = [
    '1. å‡†ç¡®è¯†åˆ«é»‘è‰²ç´ ç˜¤(MEL) - æœ€å±é™©çš„çš®è‚¤ç™Œ',
    '2. åŒºåˆ†7ç§å¸¸è§çš®è‚¤ç—…å˜',
    '3. é™ä½è¯¯è¯Šç‡ï¼Œæé«˜è¯Šæ–­æ•ˆç‡',
    '4. ä¸ºåŒ»ç”Ÿæä¾›å¯è§£é‡Šçš„è¯Šæ–­ä¾æ®'
]

# æˆåŠŸæ ‡å‡†
success_criteria = {
    'æ•´ä½“å‡†ç¡®ç‡': '> 90%',
    'MEL F1åˆ†æ•°': '> 0.85',
    'DFå¬å›ç‡': '> 30% (æå°‘æ ·æœ¬ç±»åˆ«)',
    'æ¨ç†é€Ÿåº¦': '> 20 FPS (å®æ—¶æ€§)'
}
```

### 2.2 æ•°æ®é›†åˆ†æ

#### BCN20000æ•°æ®é›†

```python
dataset_info = {
    'åç§°': 'BCN20000',
    'æ¥æº': 'Barcelonaçš®è‚¤ç—…åŒ»é™¢',
    'æ ·æœ¬æ•°': 19424,
    'ç±»åˆ«æ•°': 7,
    'å›¾åƒæ ¼å¼': 'JPG',
    'å›¾åƒå°ºå¯¸': '450Ã—600 ~ 6000Ã—4000 (ä¸ç­‰)',
    'æ ‡æ³¨è´¨é‡': 'ä¸“ä¸šçš®è‚¤ç§‘åŒ»ç”Ÿæ ‡æ³¨'
}

# ç±»åˆ«åˆ†å¸ƒï¼ˆæåº¦ä¸å¹³è¡¡ï¼‰
class_stats = """
NV:    12,875 (66.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
MEL:    3,323 (17.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
BKL:    2,624 (13.5%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
BCC:      514 (2.6%)  â–ˆ
AKIEC:     67 (0.3%)  
VASC:      15 (0.1%)  
DF:         6 (0.03%) â† æåº¦ç¨€ç¼ºï¼

ä¸å¹³è¡¡æ¯”ä¾‹: 2146:1 (NV vs DF)
"""

# æ•°æ®åˆ’åˆ†ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
data_split = {
    'è®­ç»ƒé›†': '13,597æ ·æœ¬ (70%)',
    'éªŒè¯é›†': '2,913æ ·æœ¬ (15%)',
    'æµ‹è¯•é›†': '2,914æ ·æœ¬ (15%)',
    'ç­–ç•¥': 'åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒæ¯ä¸ªsplitä¸­ç±»åˆ«æ¯”ä¾‹ä¸€è‡´'
}
```

#### HAM10000æ•°æ®é›†

```python
ham10000_info = {
    'åç§°': 'HAM10000',
    'æ¥æº': 'ç»´ä¹Ÿçº³åŒ»ç§‘å¤§å­¦',
    'æ ·æœ¬æ•°': 10015,
    'ç±»åˆ«æ•°': 7,
    'ç‰¹ç‚¹': 'å¤šæ¥æºã€å¤šè®¾å¤‡é‡‡é›†ï¼Œæ³›åŒ–æ€§å¥½'
}
```

### 2.3 è¯„ä¼°æŒ‡æ ‡

```python
# ä¸»è¦æŒ‡æ ‡
primary_metrics = {
    'å‡†ç¡®ç‡ (Accuracy)': 'æ•´ä½“åˆ†ç±»æ­£ç¡®ç‡',
    'F1 Macro': 'å„ç±»åˆ«F1çš„å¹³å‡å€¼ï¼ˆå¤„ç†ä¸å¹³è¡¡ï¼‰',
    'MEL F1': 'é»‘è‰²ç´ ç˜¤F1åˆ†æ•°ï¼ˆå…³é”®æŒ‡æ ‡ï¼‰',
    'AUC': 'å—è¯•è€…å·¥ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯'
}

# ä¸ºä»€ä¹ˆé€‰æ‹©è¿™äº›æŒ‡æ ‡ï¼Ÿ
metric_rationale = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŒ‡æ ‡        â”‚ åŸå›          â”‚ é€‚ç”¨åœºæ™¯        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å‡†ç¡®ç‡      â”‚ æ•´ä½“æ€§èƒ½     â”‚ ç±»åˆ«å¹³è¡¡æ•°æ®    â”‚
â”‚ F1 Macro    â”‚ å¹³ç­‰å¯¹å¾…æ¯ç±» â”‚ ç±»åˆ«ä¸å¹³è¡¡ â˜…    â”‚
â”‚ MEL F1      â”‚ å…³é”®ç–¾ç—…     â”‚ é«˜é£é™©åœºæ™¯ â˜…    â”‚
â”‚ å¬å›ç‡      â”‚ ä¸æ¼è¯Š       â”‚ åŒ»å­¦è¯Šæ–­ â˜…      â”‚
â”‚ AUC         â”‚ é˜ˆå€¼æ— å…³     â”‚ ç»¼åˆè¯„ä¼°        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""
```

---

## 3. è§£å†³æ–¹æ¡ˆæ¶æ„

### 3.1 æ•´ä½“æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å®Œæ•´ç³»ç»Ÿæ¶æ„                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥å±‚
â”œâ”€ çš®è‚¤é•œå›¾åƒ [224Ã—224Ã—3]
â”œâ”€ æ•°æ®å¢å¼º (æ—‹è½¬ã€ç¿»è½¬ã€é¢œè‰²æŠ–åŠ¨)
â””â”€ æ ‡å‡†åŒ– (ImageNetå‡å€¼/æ ‡å‡†å·®)
    â†“
ç‰¹å¾æå–å±‚ (Swin Transformer Backbone)
â”œâ”€ Stage 1: 56Ã—56Ã—96   â† å±€éƒ¨çº¹ç†ç‰¹å¾
â”œâ”€ Stage 2: 28Ã—28Ã—192  â† ä¸­å±‚å½¢çŠ¶ç‰¹å¾
â”œâ”€ Stage 3: 14Ã—14Ã—384  â† é«˜å±‚è¯­ä¹‰ç‰¹å¾
â”œâ”€ Stage 4: 7Ã—7Ã—768    â† å…¨å±€ä¸Šä¸‹æ–‡
â””â”€ Global Pool: 1024-d ç‰¹å¾å‘é‡
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               â”‚                 â”‚              â”‚
åˆ†ç±»å±‚ (ä¸‰ç§æ–¹æ¡ˆ)
â”‚               â”‚                 â”‚              â”‚
æ–¹æ¡ˆ1:          æ–¹æ¡ˆ2:            æ–¹æ¡ˆ3:
å•åˆ†æ”¯          åŒåˆ†æ”¯(ç®€å•)      åŒåˆ†æ”¯(æ³¨æ„åŠ›)
â”‚               â”‚                 â”‚
Dropout         é€šç”¨åˆ†æ”¯          é€šç”¨åˆ†æ”¯
Linear(7)       MELåˆ†æ”¯           MELåˆ†æ”¯
â”‚               ç®€å•èåˆ          æ³¨æ„åŠ›èåˆ
â”‚               â”‚                 â”‚
è¾“å‡º[7]         è¾“å‡º[7]           è¾“å‡º[7]
â”‚               â”‚                 â”‚
91.14%          91.38%            91.55% 
```

### 3.2 ä¸‰ç§æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | æ¶æ„ | BCN20000 | HAM10000 | BCN MEL F1 | HAM MEL F1 | å‚æ•°é‡ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|----------|------------|------------|--------|------|---------|
| **æ–¹æ¡ˆ1** | Swin-Base | 92.38% | 97.90% | 0.922 | 0.964 | 88.0M | 25 FPS | é€šç”¨åˆ†ç±» |
| **æ–¹æ¡ˆ2** | Swin+Focal | 93.24% | 90.32% | **0.976** | 0.623 | 88.2M | 25 FPS | MELæ£€æµ‹ä¼˜å…ˆ |
| **æ–¹æ¡ˆ3** | Swin Dual-Branch | **93.33%** | **ğŸ† 98.90%** | **0.974** | **ğŸ† 0.977** | 88.5M | 24 FPS | **æœ€ä½³æ€§èƒ½** |

**é€‰æ‹©å»ºè®®**:
- **é€šç”¨åˆ†ç±»**: æ–¹æ¡ˆ1ï¼ˆSwin-Baseï¼Œ97.90% HAMå‡†ç¡®ç‡ï¼‰
- **BCN MELæ£€æµ‹**: æ–¹æ¡ˆ2ï¼ˆSwin+Focalï¼Œ0.976 MEL F1ï¼‰
- **æœ€ä½³ç»¼åˆæ€§èƒ½**: æ–¹æ¡ˆ3ï¼ˆSwin Dual-Branchï¼Œ98.90% HAMå‡†ç¡®ç‡ï¼Œ96.12%å¹³å‡ï¼‰â­ æ¨è

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šç®—æ³•è¯¦è§£

## 4. Swin TransformeråŸç†

### 4.1 æ ¸å¿ƒåˆ›æ–°ï¼šç§»åŠ¨çª—å£æœºåˆ¶

#### ä¼ ç»ŸViTçš„é—®é¢˜

```python
# Vision Transformer (ViT)
# è¾“å…¥: 224Ã—224å›¾åƒ â†’ 196ä¸ªpatch (14Ã—14)
# è®¡ç®—å¤æ‚åº¦: O(nÂ²) = O(196Â²) = 38,416æ¬¡è®¡ç®—

class ViT_Attention:
    def forward(self, x):
        # å…¨å±€è‡ªæ³¨æ„åŠ›ï¼šæ‰€æœ‰patchä¹‹é—´è®¡ç®—æ³¨æ„åŠ›
        Q, K, V = self.qkv(x)  # [B, 196, 768]
        attention = softmax(Q @ K.T / sqrt(d))  # [B, 196, 196] â† å·¨å¤§ï¼
        output = attention @ V
        return output

# é—®é¢˜:
# 1. è®¡ç®—å¤æ‚åº¦O(nÂ²)ï¼Œå¯¹é«˜åˆ†è¾¨ç‡å›¾åƒä¸å‹å¥½
# 2. ç¼ºä¹å±‚æ¬¡åŒ–ç‰¹å¾ï¼Œæ— æ³•æ•è·å¤šå°ºåº¦ä¿¡æ¯
# 3. å¯¹ä¸­ç­‰è§„æ¨¡æ•°æ®é›†å®¹æ˜“è¿‡æ‹Ÿåˆ
```

#### Swinçš„æ”¹è¿›ï¼šçª—å£æ³¨æ„åŠ›

```python
# Swin Transformer
# Stage 1: 56Ã—56 â†’ åˆ’åˆ†ä¸º8Ã—8ä¸ªçª—å£ï¼Œæ¯ä¸ªçª—å£7Ã—7
# è®¡ç®—å¤æ‚åº¦: O(n) = 64 Ã— (7Ã—7)Â² = 3,136æ¬¡è®¡ç®— (å‡å°‘92%!)

class Swin_WindowAttention:
    def forward(self, x):
        # çª—å£æ³¨æ„åŠ› (W-MSA): åªåœ¨çª—å£å†…è®¡ç®—
        windows = window_partition(x, window_size=7)  # [B*64, 49, 96]
        
        for window in windows:  # 64ä¸ªçª—å£å¹¶è¡Œ
            Q, K, V = self.qkv(window)  # [B, 49, 96]
            attention = softmax(Q @ K.T / sqrt(d))  # [B, 49, 49] â† å°ï¼
            output = attention @ V
        
        return window_reverse(output)

# ä¼˜åŠ¿:
# 1. çº¿æ€§å¤æ‚åº¦O(n)
# 2. å±€éƒ¨æ„Ÿå—é‡ï¼Œç±»ä¼¼CNNçš„å½’çº³åç½®
# 3. è®¡ç®—æ•ˆç‡é«˜
```

#### ç§»åŠ¨çª—å£ï¼šè·¨çª—å£ä¿¡æ¯äº¤äº’

```python
# ç§»åŠ¨çª—å£æ³¨æ„åŠ› (SW-MSA)
class Swin_ShiftedWindowAttention:
    def forward(self, x):
        # ç¬¬1å±‚: å¸¸è§„çª—å£æ³¨æ„åŠ›
        x = W_MSA(x)
        
        # ç¬¬2å±‚: ç§»åŠ¨çª—å£æ³¨æ„åŠ›
        x_shifted = torch.roll(x, shifts=(-3, -3), dims=(1, 2))
        x = SW_MSA(x_shifted)
        x = torch.roll(x, shifts=(3, 3), dims=(1, 2))  # ç§»å›
        
        return x

# æ•ˆæœ: å®ç°è·¨çª—å£ä¿¡æ¯äº¤äº’ï¼ŒåŒæ—¶ä¿æŒçº¿æ€§å¤æ‚åº¦
```

### 4.2 å±‚æ¬¡åŒ–æ¶æ„

```
è¾“å…¥å›¾åƒ (224Ã—224Ã—3)
    â†“ Patch Partition (4Ã—4)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 1: 56Ã—56Ã—96                                       â”‚
â”‚ â”œâ”€ Swin Block Ã— 2                                       â”‚
â”‚ â”‚  â”œâ”€ W-MSA (çª—å£æ³¨æ„åŠ›)                                â”‚
â”‚ â”‚  â”œâ”€ MLP                                               â”‚
â”‚ â”‚  â”œâ”€ SW-MSA (ç§»åŠ¨çª—å£æ³¨æ„åŠ›)                           â”‚
â”‚ â”‚  â””â”€ MLP                                               â”‚
â”‚ â””â”€ ç‰¹å¾: å±€éƒ¨çº¹ç†ã€è¾¹ç•Œç»†èŠ‚                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Patch Merging (2Ã—ä¸‹é‡‡æ ·, é€šé“Ã—2)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 2: 28Ã—28Ã—192                                      â”‚
â”‚ â”œâ”€ Swin Block Ã— 2                                       â”‚
â”‚ â””â”€ ç‰¹å¾: ä¸­å±‚å½¢çŠ¶ã€è¾¹ç•Œè§„åˆ™æ€§                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Patch Merging
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 3: 14Ã—14Ã—384                                      â”‚
â”‚ â”œâ”€ Swin Block Ã— 6                                       â”‚
â”‚ â””â”€ ç‰¹å¾: é«˜å±‚è¯­ä¹‰ã€é¢œè‰²åˆ†å¸ƒ                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Patch Merging
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 4: 7Ã—7Ã—768                                        â”‚
â”‚ â”œâ”€ Swin Block Ã— 2                                       â”‚
â”‚ â””â”€ ç‰¹å¾: å…¨å±€ä¸Šä¸‹æ–‡ã€æ•´ä½“æ¨¡å¼                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ Global Average Pooling
ç‰¹å¾å‘é‡ (1024-d)
```

### 4.3 å…³é”®æŠ€æœ¯ç»†èŠ‚

#### ç›¸å¯¹ä½ç½®åç½®

```python
class WindowAttention(nn.Module):
    def __init__(self, dim, window_size, num_heads):
        super().__init__()
        self.window_size = window_size
        
        # ç›¸å¯¹ä½ç½®åç½®è¡¨
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)
        )
        
        # ç›¸å¯¹ä½ç½®ç´¢å¼•
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        self.register_buffer("relative_position_index", relative_coords.sum(-1))
    
    def forward(self, x):
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        # è®¡ç®—æ³¨æ„åŠ›
        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))
        
        # æ·»åŠ ç›¸å¯¹ä½ç½®åç½®
        relative_position_bias = self.relative_position_bias_table[
            self.relative_position_index.view(-1)
        ].view(self.window_size[0] * self.window_size[1], 
               self.window_size[0] * self.window_size[1], -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)
        
        attn = self.softmax(attn)
        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        return x
```

---

## 5. Focal Lossæœºåˆ¶

### 5.1 äº¤å‰ç†µæŸå¤±çš„é—®é¢˜

```python
# æ ‡å‡†äº¤å‰ç†µæŸå¤±
class CrossEntropyLoss:
    def forward(self, logits, targets):
        # å¯¹æ‰€æœ‰æ ·æœ¬å¹³ç­‰å¯¹å¾…
        loss = -log(p_t)
        return loss.mean()

# é—®é¢˜ç¤ºä¾‹
example = """
å‡è®¾batchä¸­æœ‰100ä¸ªæ ·æœ¬:
- 90ä¸ªNVæ ·æœ¬ (æ˜“åˆ†ç±»): loss = 0.01 each
- 10ä¸ªDFæ ·æœ¬ (éš¾åˆ†ç±»): loss = 2.5 each

æ€»æŸå¤± = (90 Ã— 0.01 + 10 Ã— 2.5) / 100 = 0.259
æ¢¯åº¦ä¸»è¦æ¥è‡ª: 90ä¸ªæ˜“åˆ†æ ·æœ¬ (0.9) vs 10ä¸ªéš¾åˆ†æ ·æœ¬ (25.0)

ç»“æœ: æ¨¡å‹è¢«æ˜“åˆ†æ ·æœ¬ä¸»å¯¼ï¼Œéš¾åˆ†æ ·æœ¬å­¦ä¸åˆ°ï¼
"""
```

### 5.2 Focal LossåŸç†

#### æ•°å­¦å…¬å¼

```python
# Focal Losså…¬å¼
FL(p_t) = -Î±_t(1-p_t)^Î³ log(p_t)

# å…¶ä¸­:
# p_t: çœŸå®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡
# Î±_t: ç±»åˆ«æƒé‡ (é€šå¸¸0.25)
# Î³: èšç„¦å‚æ•° (é€šå¸¸2.0)
# (1-p_t)^Î³: è°ƒåˆ¶å› å­ (modulating factor)
```

#### å·¥ä½œåŸç†

```python
# è°ƒåˆ¶å› å­çš„ä½œç”¨
modulating_factor_effect = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ ·æœ¬ç±»å‹ â”‚ p_t        â”‚ (1-p_t)^2    â”‚ æƒé‡å˜åŒ–   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ˜“åˆ†æ ·æœ¬ â”‚ 0.9        â”‚ 0.01         â”‚ â†“ 99%      â”‚
â”‚ ä¸­ç­‰æ ·æœ¬ â”‚ 0.7        â”‚ 0.09         â”‚ â†“ 91%      â”‚
â”‚ éš¾åˆ†æ ·æœ¬ â”‚ 0.5        â”‚ 0.25         â”‚ â†“ 75%      â”‚
â”‚ å¾ˆéš¾æ ·æœ¬ â”‚ 0.3        â”‚ 0.49         â”‚ â†“ 51%      â”‚
â”‚ æéš¾æ ·æœ¬ â”‚ 0.1        â”‚ 0.81         â”‚ â†“ 19%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ•ˆæœ: è‡ªåŠ¨é™ä½æ˜“åˆ†æ ·æœ¬æƒé‡ï¼Œèšç„¦éš¾åˆ†æ ·æœ¬ï¼
"""
```

### 5.3 å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FocalLoss(nn.Module):
    """
    Focal Losså®ç°
    
    è®ºæ–‡: "Focal Loss for Dense Object Detection" (Lin et al., ICCV 2017)
    
    å‚æ•°:
        alpha: ç±»åˆ«æƒé‡ï¼Œå¹³è¡¡æ­£è´Ÿæ ·æœ¬ (default: 0.25)
        gamma: èšç„¦å‚æ•°ï¼Œæ§åˆ¶éš¾æ˜“æ ·æœ¬æƒé‡ (default: 2.0)
        reduction: 'mean' | 'sum' | 'none'
    
    å·¥ä½œåŸç†:
        æ˜“åˆ†æ ·æœ¬ (p_t=0.9): (1-0.9)^2 = 0.01 â†’ æƒé‡é™ä½99%
        éš¾åˆ†æ ·æœ¬ (p_t=0.3): (1-0.3)^2 = 0.49 â†’ æƒé‡ä¿æŒé«˜ä½
    """
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
    
    def forward(self, inputs, targets):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: æ¨¡å‹è¾“å‡ºlogits [batch_size, num_classes]
            targets: çœŸå®æ ‡ç­¾ [batch_size]
        
        Returns:
            loss: Focal Losså€¼
        """
        # 1. è®¡ç®—æ ‡å‡†äº¤å‰ç†µæŸå¤±ï¼ˆä¸åšreductionï¼‰
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        # ce_loss shape: [batch_size]
        
        # 2. è®¡ç®—p_tï¼ˆçœŸå®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ï¼‰
        pt = torch.exp(-ce_loss)  # pt = exp(-log(pt)) = pt
        # pt shape: [batch_size]
        
        # 3. åº”ç”¨Focal Losså…¬å¼
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        # (1-pt)^gamma: è°ƒåˆ¶å› å­ï¼Œæ˜“åˆ†æ ·æœ¬â†’0ï¼Œéš¾åˆ†æ ·æœ¬â†’1
        
        # 4. Reduction
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# ä½¿ç”¨ç¤ºä¾‹
criterion = FocalLoss(alpha=0.25, gamma=2.0)
loss = criterion(outputs, labels)
```

### 5.4 å‚æ•°é€‰æ‹©æŒ‡å—

```python
# Î³å‚æ•°çš„å½±å“
gamma_effects = """
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Î³å€¼ â”‚ æ•ˆæœ         â”‚ é€‚ç”¨åœºæ™¯            â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 0   â”‚ ç­‰åŒäºCE     â”‚ ç±»åˆ«å¹³è¡¡æ•°æ®        â”‚
â”‚ 0.5 â”‚ è½»å¾®èšç„¦     â”‚ è½»åº¦ä¸å¹³è¡¡(10:1)    â”‚
â”‚ 1.0 â”‚ ä¸­åº¦èšç„¦     â”‚ ä¸­åº¦ä¸å¹³è¡¡(50:1)    â”‚
â”‚ 2.0 â”‚ æ ‡å‡†èšç„¦ â˜…   â”‚ é‡åº¦ä¸å¹³è¡¡(100:1) â˜… â”‚
â”‚ 3.0 â”‚ å¼ºèšç„¦       â”‚ æåº¦ä¸å¹³è¡¡(1000:1)  â”‚
â”‚ 5.0 â”‚ æå¼ºèšç„¦     â”‚ å¯èƒ½è¿‡æ‹Ÿåˆéš¾æ ·æœ¬    â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¨è: Î³=2.0 (Lin et al. åŸè®ºæ–‡æ¨èå€¼)
"""

# Î±å‚æ•°çš„å½±å“
alpha_effects = """
Î±å‚æ•°ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬:
- Î±=0.25: é€‚åˆæ­£æ ·æœ¬è¾ƒå°‘çš„æƒ…å†µ
- Î±=0.5: æ­£è´Ÿæ ·æœ¬å¹³è¡¡
- Î±=0.75: é€‚åˆè´Ÿæ ·æœ¬è¾ƒå°‘çš„æƒ…å†µ

å¯¹äºå¤šåˆ†ç±»ï¼Œé€šå¸¸ä½¿ç”¨å›ºå®šå€¼Î±=0.25
"""

# æˆ‘ä»¬çš„æœ€ä½³é…ç½®
best_config = {
    'alpha': 0.25,
    'gamma': 2.0,
    'ä¸å¹³è¡¡æ¯”ä¾‹': '2146:1 (NV vs DF)',
    'æ€§èƒ½æå‡': '+3.16% (vs CrossEntropy)'
}
```

### 5.5 å®éªŒå¯¹æ¯”

```python
# ä¸åŒæŸå¤±å‡½æ•°çš„æ€§èƒ½å¯¹æ¯”
loss_comparison = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æŸå¤±å‡½æ•°         â”‚ BCN20000 â”‚ HAM10000 â”‚ MEL F1  â”‚ DFå¬å›ç‡ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CrossEntropy     â”‚ 87.12%   â”‚ 88.42%   â”‚ 0.7234  â”‚ 0%       â”‚
â”‚ Weighted CE      â”‚ 88.45%   â”‚ 89.67%   â”‚ 0.7456  â”‚ 12%      â”‚
â”‚ Focal Loss â˜…     â”‚ 91.14%   â”‚ 92.52%   â”‚ 0.8234  â”‚ 45%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®å‘ç°:
1. Focal Lossåœ¨æåº¦ä¸å¹³è¡¡æ•°æ®ä¸Šè¡¨ç°æœ€ä½³
2. DFç±»åˆ«ä»0%å¬å›ç‡æå‡åˆ°45%
3. MEL F1æå‡13.8% (0.7234 â†’ 0.8234)
"""
```

---

## 6. åŒåˆ†æ”¯æ¶æ„è®¾è®¡

### 6.1 è®¾è®¡åŠ¨æœº

#### åŒ»å­¦è¯Šæ–­çš„ç‰¹æ®Šéœ€æ±‚

```python
# é»‘è‰²ç´ ç˜¤(MEL)çš„ç‰¹æ®Šæ€§
melanoma_challenges = {
    'å±é™©æ€§': {
        'æè¿°': 'æœ€è‡´å‘½çš„çš®è‚¤ç™Œ',
        'æ•°æ®': '5å¹´ç”Ÿå­˜ç‡å–å†³äºæ—©æœŸå‘ç°',
        'å½±å“': 'æ¼è¯Šå¯èƒ½å±åŠç”Ÿå‘½'
    },
    'æ··æ·†æ€§': {
        'æè¿°': 'ä¸è‰¯æ€§ç—£(NV)å¤–è§‚ç›¸ä¼¼',
        'æ•°æ®': 'MELå’ŒNVå æ€»æ ·æœ¬83.4%',
        'å½±å“': 'å®¹æ˜“è¯¯è¯Šï¼Œéœ€è¦ä¸“é—¨åˆ¤åˆ«èƒ½åŠ›'
    },
    'æ ·æœ¬é‡': {
        'æè¿°': 'å 17.1%ï¼Œç›¸å¯¹è¾ƒå°‘ä½†æå…¶é‡è¦',
        'æ•°æ®': '3,323ä¸ªMELæ ·æœ¬ vs 12,875ä¸ªNVæ ·æœ¬',
        'å½±å“': 'éœ€è¦ç‰¹åˆ«å…³æ³¨ï¼Œæé«˜æ•æ„Ÿåº¦'
    }
}

# å•åˆ†æ”¯æ¨¡å‹çš„å±€é™
single_branch_limitations = """
é—®é¢˜1: ç»Ÿä¸€å¤„ç†
- æ‰€æœ‰7ç±»ç–¾ç—…å¹³ç­‰å¯¹å¾…
- MELç‰¹å¾ä¸å…¶ä»–6ç±»ç«äº‰
- å…³é”®ç–¾ç—…æœªå¾—åˆ°ç‰¹åˆ«å…³æ³¨

é—®é¢˜2: ç‰¹å¾ç«äº‰
- å…±äº«åˆ†ç±»å™¨å‚æ•°
- MELç‰¹å¾å¯èƒ½è¢«NVç‰¹å¾å‹åˆ¶
- éš¾ä»¥å­¦ä¹ MELçš„ç»†å¾®åˆ¤åˆ«ç‰¹å¾

é—®é¢˜3: æ•æ„Ÿåº¦ä¸è¶³
- MELå¬å›ç‡å¯èƒ½ä¸å¤Ÿé«˜
- æ¼è¯Šé£é™©è¾ƒå¤§
- æ— æ³•æ»¡è¶³åŒ»å­¦è¯Šæ–­éœ€æ±‚
"""

# åŒåˆ†æ”¯è§£å†³æ–¹æ¡ˆ
dual_branch_solution = """
è®¾è®¡æ€è·¯:
1. é€šç”¨åˆ†æ”¯: å¤„ç†æ‰€æœ‰7ç±»ç–¾ç—…çš„åˆ†ç±»
2. ä¸“é¡¹åˆ†æ”¯: ä¸“æ³¨äºMELæ£€æµ‹ï¼ˆMEL vs éMELï¼‰
3. æ³¨æ„åŠ›èåˆ: åŠ¨æ€è°ƒæ•´ä¸¤ä¸ªåˆ†æ”¯çš„è´¡çŒ®

ä¼˜åŠ¿:
1. ä¸“é—¨çš„MELåˆ¤åˆ«èƒ½åŠ›
2. å¤šä»»åŠ¡å­¦ä¹ ï¼Œç‰¹å¾æ›´ä¸°å¯Œ
3. æ³¨æ„åŠ›æœºåˆ¶ï¼Œè‡ªé€‚åº”èåˆ
4. MEL F1æå‡4.6% (0.8234 â†’ 0.8612)
"""
```

### 6.2 æ¶æ„è®¾è®¡

```
è¾“å…¥å›¾åƒ [B, 3, 224, 224]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Swin Transformer Backbone (å…±äº«ç‰¹å¾æå–å™¨)           â”‚
â”‚  â”œâ”€ Stage 1: 56Ã—56Ã—96   (å±€éƒ¨çº¹ç†)                   â”‚
â”‚  â”œâ”€ Stage 2: 28Ã—28Ã—192  (ä¸­å±‚å½¢çŠ¶)                   â”‚
â”‚  â”œâ”€ Stage 3: 14Ã—14Ã—384  (é«˜å±‚è¯­ä¹‰)                   â”‚
â”‚  â”œâ”€ Stage 4: 7Ã—7Ã—768    (å…¨å±€ä¸Šä¸‹æ–‡)                 â”‚
â”‚  â””â”€ Global Pool: 1024-d                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   features [B, 1024]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               â”‚               â”‚              â”‚
â”‚  é€šç”¨åˆ†æ”¯      â”‚  MELä¸“é¡¹åˆ†æ”¯   â”‚  æ³¨æ„åŠ›æ¨¡å—   â”‚
â”‚  (7åˆ†ç±»)      â”‚  (2åˆ†ç±»)      â”‚  (æƒé‡è®¡ç®—)  â”‚
â”‚               â”‚               â”‚              â”‚
â”‚  Dropout(0.3) â”‚  Dropout(0.3) â”‚  Linear(128) â”‚
â”‚  Linear(512)  â”‚  Linear(256)  â”‚  ReLU        â”‚
â”‚  ReLU         â”‚  ReLU         â”‚  Linear(2)   â”‚
â”‚  Dropout(0.4) â”‚  Dropout(0.4) â”‚  Softmax     â”‚
â”‚  Linear(7)    â”‚  Linear(2)    â”‚              â”‚
â”‚               â”‚               â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“               â†“              â†“
  general_logits  melanoma_logits  attention_weights
     [B, 7]          [B, 2]          [B, 2]
        â†“               â†“              â†“
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  æ³¨æ„åŠ›èåˆæ¨¡å—          â”‚
              â”‚  1. æå–MELæ¦‚ç‡         â”‚
              â”‚  2. è®¡ç®—å¢å¼ºå¼ºåº¦        â”‚
              â”‚  3. åŠ¨æ€å¢å¼ºMELç±»åˆ«     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
              enhanced_logits [B, 7]
```

### 6.3 å®Œæ•´ä»£ç å®ç°

```python
import torch
import torch.nn as nn
import timm

class SwinDualBranchAttentionModel(nn.Module):
    """
    Swin TransformeråŒåˆ†æ”¯æ¨¡å‹ + æ³¨æ„åŠ›èåˆ

    è®¾è®¡åŠ¨æœº:
        é»‘è‰²ç´ ç˜¤(MEL)æ˜¯æœ€å±é™©çš„çš®è‚¤ç™Œï¼Œéœ€è¦ä¸“é—¨æ£€æµ‹
        é€šç”¨åˆ†æ”¯å¤„ç†æ‰€æœ‰7ç±»ï¼Œä¸“é¡¹åˆ†æ”¯ä¸“æ³¨MELæ£€æµ‹
        æ³¨æ„åŠ›èåˆåŠ¨æ€è°ƒæ•´ä¸¤ä¸ªåˆ†æ”¯çš„è´¡çŒ®

    æ¶æ„:
        - å…±äº«Backbone: Swin-Base (88Må‚æ•°)
        - é€šç”¨åˆ†æ”¯: 7åˆ†ç±» (NV, MEL, BKL, BCC, AKIEC, VASC, DF)
        - MELä¸“é¡¹åˆ†æ”¯: 2åˆ†ç±» (MEL vs éMEL)
        - æ³¨æ„åŠ›æ¨¡å—: åŠ¨æ€æƒé‡è®¡ç®—

    æ€§èƒ½ï¼ˆæœ€æ–°è¯„ä¼°ç»“æœï¼‰:
        - BCN20000: 93.33% (vs Swin-Base 92.38%, +0.95%)
        - HAM10000: 98.90% (vs Swin-Base 97.90%, +1.00%)
        - BCN MEL F1: 0.974 (vs Swin-Base 0.922, +5.6%)
        - HAM MEL F1: 0.977 (vs Swin-Base 0.964, +1.3%)
        - å¹³å‡å‡†ç¡®ç‡: 96.12% (æ‰€æœ‰æ¨¡å‹ä¸­æœ€é«˜)

    å‚æ•°é‡: 88.5M (+0.5M vs Swin-Base)
    æ¨ç†é€Ÿåº¦: 24 FPS (-1 FPS vs Swin-Base)
    """
    def __init__(self, num_classes=7):
        super().__init__()

        # ========== å…±äº«ç‰¹å¾æå–å™¨ ==========
        self.backbone = timm.create_model(
            'swin_base_patch4_window7_224',
            pretrained=True,  # ImageNet-1Ké¢„è®­ç»ƒ
            num_classes=0,    # ç§»é™¤åŸå§‹åˆ†ç±»å¤´
            global_pool='avg' # å…¨å±€å¹³å‡æ± åŒ–
        )

        feature_dim = self.backbone.num_features  # 1024

        # ========== é€šç”¨åˆ†æ”¯ (7åˆ†ç±») ==========
        # å¤„ç†æ‰€æœ‰7ç§çš®è‚¤ç—…å˜çš„åˆ†ç±»
        self.general_branch = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(feature_dim, 512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(512, num_classes)
        )

        # ========== é»‘è‰²ç´ ç˜¤ä¸“é¡¹åˆ†æ”¯ (2åˆ†ç±») ==========
        # ä¸“æ³¨äºMELæ£€æµ‹: MEL vs éMEL
        self.melanoma_branch = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(feature_dim, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(256, 2)  # MEL vs éMEL
        )

        # ========== æ³¨æ„åŠ›èåˆæ¨¡å— ==========
        # åŠ¨æ€è®¡ç®—ä¸¤ä¸ªåˆ†æ”¯çš„æƒé‡
        self.attention = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(inplace=True),
            nn.Linear(128, 2),
            nn.Softmax(dim=1)
        )

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: è¾“å…¥å›¾åƒ [batch_size, 3, 224, 224]

        Returns:
            enhanced_logits: èåˆåçš„åˆ†ç±»logits [batch_size, 7]

        æµç¨‹:
            1. å…±äº«ç‰¹å¾æå–
            2. ä¸¤ä¸ªåˆ†æ”¯å¹¶è¡Œè®¡ç®—
            3. æ³¨æ„åŠ›æƒé‡è®¡ç®—
            4. åŠ¨æ€èåˆ
        """
        # 1. å…±äº«ç‰¹å¾æå–
        features = self.backbone(x)  # [B, 1024]

        # 2. ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡º
        general_logits = self.general_branch(features)    # [B, 7]
        melanoma_logits = self.melanoma_branch(features)  # [B, 2]

        # 3. æ³¨æ„åŠ›æƒé‡è®¡ç®—
        attention_weights = self.attention(features)  # [B, 2]
        # attention_weights[:, 0]: é€šç”¨åˆ†æ”¯æƒé‡
        # attention_weights[:, 1]: MELåˆ†æ”¯æƒé‡

        # 4. èåˆç­–ç•¥
        # 4.1 æå–MELçš„é¢„æµ‹æ¦‚ç‡
        melanoma_prob = torch.softmax(melanoma_logits, dim=1)[:, 1:2]  # [B, 1]

        # 4.2 å¤åˆ¶é€šç”¨åˆ†æ”¯çš„è¾“å‡º
        enhanced_logits = general_logits.clone()

        # 4.3 ä½¿ç”¨æ³¨æ„åŠ›æƒé‡åŠ¨æ€å¢å¼ºMELç±»åˆ«çš„é¢„æµ‹
        enhancement_strength = attention_weights[:, 1:2] * 3.0  # [B, 1]
        enhanced_logits[:, 4:5] += melanoma_prob * enhancement_strength
        # æ³¨: MELæ˜¯ç¬¬4ç±»ï¼ˆç´¢å¼•ä»0å¼€å§‹ï¼‰

        return enhanced_logits

    def get_dual_outputs(self, x):
        """
        è·å–ä¸¤ä¸ªåˆ†æ”¯çš„åŸå§‹è¾“å‡ºï¼ˆç”¨äºè®­ç»ƒæ—¶çš„å¤šä»»åŠ¡æŸå¤±ï¼‰

        Args:
            x: è¾“å…¥å›¾åƒ [batch_size, 3, 224, 224]

        Returns:
            general_logits: é€šç”¨åˆ†æ”¯è¾“å‡º [B, 7]
            melanoma_logits: MELåˆ†æ”¯è¾“å‡º [B, 2]
        """
        features = self.backbone(x)
        general_logits = self.general_branch(features)
        melanoma_logits = self.melanoma_branch(features)
        return general_logits, melanoma_logits

# ä½¿ç”¨ç¤ºä¾‹
model = SwinDualBranchAttentionModel(num_classes=7)
model = model.cuda()

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
print(f"æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
# è¾“å‡º: æ€»å‚æ•°é‡: 88.5M

# æ¨ç†
images = torch.randn(8, 3, 224, 224).cuda()
outputs = model(images)  # [8, 7]
print(f"è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
```

### 6.4 å¤šä»»åŠ¡æŸå¤±å‡½æ•°

```python
def dual_branch_loss(model, images, labels, lambda_mel=0.5):
    """
    åŒåˆ†æ”¯æ¨¡å‹çš„å¤šä»»åŠ¡æŸå¤±å‡½æ•°

    ç»„åˆä¸¤ä¸ªä»»åŠ¡çš„æŸå¤±:
    1. é€šç”¨åˆ†ç±»ä»»åŠ¡ (7åˆ†ç±»)
    2. é»‘è‰²ç´ ç˜¤æ£€æµ‹ä»»åŠ¡ (2åˆ†ç±»)

    Args:
        model: åŒåˆ†æ”¯æ¨¡å‹
        images: è¾“å…¥å›¾åƒ [B, 3, 224, 224]
        labels: æ ‡ç­¾ [B], å€¼èŒƒå›´0-6
        lambda_mel: MELä»»åŠ¡çš„æƒé‡ç³»æ•° (default: 0.5)

    Returns:
        total_loss: æ€»æŸå¤±
        loss_general: é€šç”¨åˆ†ç±»æŸå¤±
        loss_melanoma: MELæ£€æµ‹æŸå¤±
    """
    # è·å–ä¸¤ä¸ªåˆ†æ”¯çš„è¾“å‡º
    general_logits, melanoma_logits = model.get_dual_outputs(images)

    # ä»»åŠ¡1: é€šç”¨åˆ†ç±»æŸå¤± (7åˆ†ç±»)
    focal_loss = FocalLoss(alpha=0.25, gamma=2.0)
    loss_general = focal_loss(general_logits, labels)

    # ä»»åŠ¡2: MELæ£€æµ‹æŸå¤± (2åˆ†ç±»)
    # å°†7åˆ†ç±»æ ‡ç­¾è½¬æ¢ä¸º2åˆ†ç±»: MEL(ç±»åˆ«4) vs éMEL
    is_melanoma = (labels == 4).long()  # [B], å€¼ä¸º0æˆ–1
    loss_melanoma = focal_loss(melanoma_logits, is_melanoma)

    # ç»„åˆæŸå¤±
    total_loss = loss_general + lambda_mel * loss_melanoma

    return total_loss, loss_general, loss_melanoma

# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()

    # è®¡ç®—å¤šä»»åŠ¡æŸå¤±
    total_loss, loss_gen, loss_mel = dual_branch_loss(
        model, images, labels, lambda_mel=0.5
    )

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

    # æ—¥å¿—
    print(f"Loss: Total={total_loss:.4f}, General={loss_gen:.4f}, MEL={loss_mel:.4f}")
```

### 6.5 æ€§èƒ½å¯¹æ¯”

```python
# ä¸‰ç§æ–¹æ¡ˆçš„è¯¦ç»†å¯¹æ¯”ï¼ˆæœ€æ–°è¯„ä¼°ç»“æœï¼‰
performance_comparison = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ¡ˆ             â”‚ BCN20000 â”‚ HAM10000 â”‚ BCN MEL F1â”‚ HAM MEL F1â”‚ å‚æ•°é‡ â”‚ é€Ÿåº¦   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Swin-Base        â”‚ 92.38%   â”‚ 97.90%   â”‚ 0.922     â”‚ 0.964     â”‚ 88.0M  â”‚ 25 FPS â”‚
â”‚ Swin + Focal     â”‚ 93.24%   â”‚ 90.32%   â”‚ 0.976     â”‚ 0.623     â”‚ 88.2M  â”‚ 25 FPS â”‚
â”‚ Swin Dual-Branch â”‚ 93.33%   â”‚ 98.90%   â”‚ 0.974     â”‚ 0.977     â”‚ 88.5M  â”‚ 24 FPS â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ vs Swin-Base     â”‚ +0.95%   â”‚ +1.00%   â”‚ +5.6%     â”‚ +1.3%     â”‚ +0.6%  â”‚ -4%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®å‘ç°:
1. Swin Dual-Branchåœ¨HAM10000ä¸Šè¾¾åˆ°98.90%ï¼ˆæ¥è¿‘å®Œç¾ï¼‰
2. HAM MEL F1ä»0.964æå‡åˆ°0.977 (+1.3%)
3. å¹³å‡å‡†ç¡®ç‡96.12%ï¼Œæ‰€æœ‰æ¨¡å‹ä¸­æœ€é«˜
4. å‚æ•°å¢åŠ æå°‘ (+0.6%)
5. é€Ÿåº¦å½±å“å¯æ¥å— (-4%)
"""

# ä¸è¿‘æœŸSOTAåŒåˆ†æ”¯æ–¹æ³•å¯¹æ¯”
sota_comparison = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ³•         â”‚ å¹´ä»½ â”‚ åŒåˆ†æ”¯è®¾è®¡   â”‚ æ€§èƒ½æå‡ â”‚ å‚æ•°å¼€é”€ â”‚ åº”ç”¨åœºæ™¯ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DAX-Net      â”‚ 2024 â”‚ åŒä»»åŠ¡å­¦ä¹    â”‚ +2.3%    â”‚ +15%     â”‚ ç—…ç†å›¾åƒ â”‚
â”‚ DBTU-Net     â”‚ 2024 â”‚ Trans+U-Net  â”‚ +1.8%    â”‚ +20%     â”‚ ç—…å˜åˆ†å‰² â”‚
â”‚ Quantum      â”‚ 2024 â”‚ é‡å­+ç»å…¸    â”‚ +1.5%    â”‚ +50%     â”‚ çš®è‚¤ç™Œ   â”‚
â”‚ EDB-Net      â”‚ 2024 â”‚ è¾¹ç¼˜+è¯­ä¹‰    â”‚ +2.0%    â”‚ +20%     â”‚ ç—…å˜åˆ†ç±» â”‚
â”‚ Ours â˜…       â”‚ 2025 â”‚ é€šç”¨+ä¸“é¡¹    â”‚ +0.52%   â”‚ +0.3%    â”‚ ç—…å˜åˆ†ç±» â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æˆ‘ä»¬çš„ä¼˜åŠ¿:
1. å‚æ•°æ•ˆç‡æœ€é«˜ (ä»…+0.3%)
2. è®¾è®¡ç®€æ´ï¼Œæ˜“äºç†è§£å’Œå®ç°
3. åŒ»å­¦å¯¼å‘ï¼Œé’ˆå¯¹å…³é”®ç–¾ç—…(MEL)ä¼˜åŒ–
4. ä¸éœ€è¦ç‰¹æ®Šç¡¬ä»¶
"""
```

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šå®Œæ•´è®­ç»ƒæµç¨‹

## 7. æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†

### 7.1 æ•°æ®åŠ è½½

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
import os

class SkinLesionDataset(Dataset):
    """
    çš®è‚¤ç—…å˜æ•°æ®é›†

    æ•°æ®é›†ç»“æ„:
        data/
        â”œâ”€â”€ BCN20000/
        â”‚   â”œâ”€â”€ images/
        â”‚   â”‚   â”œâ”€â”€ ISIC_0000001.jpg
        â”‚   â”‚   â”œâ”€â”€ ISIC_0000002.jpg
        â”‚   â”‚   â””â”€â”€ ...
        â”‚   â””â”€â”€ metadata.csv
        â””â”€â”€ HAM10000/
            â”œâ”€â”€ images/
            â””â”€â”€ metadata.csv

    metadata.csvæ ¼å¼:
        image_id, diagnosis, age, sex, localization
        ISIC_0000001, NV, 45, male, back
        ISIC_0000002, MEL, 60, female, face
        ...
    """
    def __init__(self, csv_file, img_dir, transform=None):
        """
        Args:
            csv_file: å…ƒæ•°æ®CSVæ–‡ä»¶è·¯å¾„
            img_dir: å›¾åƒæ–‡ä»¶å¤¹è·¯å¾„
            transform: æ•°æ®å¢å¼º/é¢„å¤„ç†
        """
        self.df = pd.read_csv(csv_file)
        self.img_dir = img_dir
        self.transform = transform

        # ç±»åˆ«æ˜ å°„
        self.class_to_idx = {
            'NV': 0,    # è‰¯æ€§ç—£
            'MEL': 4,   # é»‘è‰²ç´ ç˜¤ (ç´¢å¼•4ï¼Œæ–¹ä¾¿åŒåˆ†æ”¯å¤„ç†)
            'BKL': 1,   # è„‚æº¢æ€§è§’åŒ–
            'BCC': 2,   # åŸºåº•ç»†èƒç™Œ
            'AKIEC': 3, # å…‰åŒ–æ€§è§’åŒ–
            'VASC': 5,  # è¡€ç®¡ç—…å˜
            'DF': 6     # çš®è‚¤çº¤ç»´ç˜¤
        }

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # è¯»å–å›¾åƒ
        img_name = self.df.iloc[idx]['image_id'] + '.jpg'
        img_path = os.path.join(self.img_dir, img_name)
        image = Image.open(img_path).convert('RGB')

        # è·å–æ ‡ç­¾
        diagnosis = self.df.iloc[idx]['diagnosis']
        label = self.class_to_idx[diagnosis]

        # æ•°æ®å¢å¼º/é¢„å¤„ç†
        if self.transform:
            image = self.transform(image)

        return image, label

# æ•°æ®å¢å¼ºç­–ç•¥
train_transform = transforms.Compose([
    # å‡ ä½•å˜æ¢
    transforms.Resize((256, 256)),
    transforms.RandomCrop(224),
    transforms.RandomHorizontalFlip(0.5),
    transforms.RandomVerticalFlip(0.5),
    transforms.RandomRotation(20),

    # é¢œè‰²å¢å¼ºï¼ˆè½»å¾®ï¼Œä¿æŒç—…å˜ç‰¹å¾ï¼‰
    transforms.ColorJitter(
        brightness=0.2,  # äº®åº¦Â±20%
        contrast=0.2,    # å¯¹æ¯”åº¦Â±20%
        saturation=0.2,  # é¥±å’Œåº¦Â±20%
        hue=0.1          # è‰²è°ƒÂ±10%
    ),

    # è½¬æ¢ä¸ºTensorå¹¶æ ‡å‡†åŒ–
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],  # ImageNetå‡å€¼
        std=[0.229, 0.224, 0.225]    # ImageNetæ ‡å‡†å·®
    )
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# åˆ›å»ºæ•°æ®é›†
train_dataset = SkinLesionDataset(
    csv_file='data/BCN20000/train.csv',
    img_dir='data/BCN20000/images',
    transform=train_transform
)

val_dataset = SkinLesionDataset(
    csv_file='data/BCN20000/val.csv',
    img_dir='data/BCN20000/images',
    transform=val_transform
)

# åˆ›å»ºDataLoader
train_loader = DataLoader(
    train_dataset,
    batch_size=64,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=64,
    shuffle=False,
    num_workers=4,
    pin_memory=True
)
```

### 7.2 æ•°æ®æ¢ç´¢ä¸åˆ†æ

```python
import matplotlib.pyplot as plt
import seaborn as sns

def analyze_dataset(csv_file):
    """æ•°æ®é›†åˆ†æ"""
    df = pd.read_csv(csv_file)

    # 1. ç±»åˆ«åˆ†å¸ƒ
    class_counts = df['diagnosis'].value_counts()
    print("ç±»åˆ«åˆ†å¸ƒ:")
    print(class_counts)
    print(f"\nä¸å¹³è¡¡æ¯”ä¾‹: {class_counts.max() / class_counts.min():.1f}:1")

    # 2. å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    sns.barplot(x=class_counts.index, y=class_counts.values)
    plt.title('Class Distribution')
    plt.xlabel('Diagnosis')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig('class_distribution.png')

    # 3. å¹´é¾„åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    df['age'].hist(bins=50)
    plt.title('Age Distribution')
    plt.xlabel('Age')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.savefig('age_distribution.png')

    # 4. æ€§åˆ«åˆ†å¸ƒ
    sex_counts = df['sex'].value_counts()
    print(f"\næ€§åˆ«åˆ†å¸ƒ:")
    print(sex_counts)

    # 5. ç—…å˜ä½ç½®åˆ†å¸ƒ
    loc_counts = df['localization'].value_counts()
    print(f"\nç—…å˜ä½ç½®åˆ†å¸ƒ:")
    print(loc_counts.head(10))

# è¿è¡Œåˆ†æ
analyze_dataset('data/BCN20000/train.csv')
```

---

## 8. æ¨¡å‹è®¾è®¡ä¸å®ç°

### 8.1 å•åˆ†æ”¯æ¨¡å‹ï¼ˆåŸºç¡€æ–¹æ¡ˆï¼‰

```python
class SwinSingleBranchModel(nn.Module):
    """
    Swin Transformerå•åˆ†æ”¯æ¨¡å‹

    æ¶æ„:
        è¾“å…¥ â†’ Swin Backbone â†’ Global Pool â†’ Dropout â†’ Linear â†’ è¾“å‡º

    æ€§èƒ½:
        - BCN20000: 91.14%
        - HAM10000: 92.52%
        - MEL F1: 0.8234

    å‚æ•°é‡: 88.2M
    æ¨ç†é€Ÿåº¦: 25 FPS

    é€‚ç”¨åœºæ™¯:
        - è¿½æ±‚ç®€æ´æ€§
        - é€šç”¨åˆ†ç±»ä»»åŠ¡
        - å¿«é€Ÿéƒ¨ç½²
    """
    def __init__(self, num_classes=7, dropout=0.5):
        super().__init__()

        # Backbone: Swin-Base (é¢„è®­ç»ƒ)
        self.backbone = timm.create_model(
            'swin_base_patch4_window7_224',
            pretrained=True,  # ImageNet-1Ké¢„è®­ç»ƒ
            num_classes=0,
            global_pool='avg'
        )

        # Classifier
        self.classifier = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(1024, num_classes)
        )

    def forward(self, x):
        features = self.backbone(x)  # [B, 1024]
        logits = self.classifier(features)  # [B, 7]
        return logits

# ä½¿ç”¨ç¤ºä¾‹
model = SwinSingleBranchModel(num_classes=7)
model = model.cuda()
print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
```

### 8.2 æ¨¡å‹åˆå§‹åŒ–ç­–ç•¥

```python
def initialize_model(model, pretrained=True):
    """
    æ¨¡å‹åˆå§‹åŒ–

    ç­–ç•¥:
        1. Backbone: ImageNeté¢„è®­ç»ƒæƒé‡
        2. åˆ†ç±»å¤´: Xavieråˆå§‹åŒ–
        3. Batch Norm: é»˜è®¤åˆå§‹åŒ–
    """
    if pretrained:
        # Backboneå·²ç»åŠ è½½é¢„è®­ç»ƒæƒé‡
        print("âœ“ Backbone: ImageNeté¢„è®­ç»ƒæƒé‡")

    # åˆå§‹åŒ–åˆ†ç±»å¤´
    for m in model.classifier.modules():
        if isinstance(m, nn.Linear):
            nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
            print(f"âœ“ Linearå±‚: Xavieråˆå§‹åŒ–")

    return model

model = initialize_model(model, pretrained=True)
```

---

## 9. è®­ç»ƒé…ç½®ä¸ä¼˜åŒ–

### 9.1 ä¼˜åŒ–å™¨é…ç½®

```python
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR

# AdamWä¼˜åŒ–å™¨ï¼ˆæ¨èï¼‰
optimizer = AdamW(
    model.parameters(),
    lr=1e-4,           # å­¦ä¹ ç‡
    weight_decay=1e-4, # L2æ­£åˆ™åŒ–
    betas=(0.9, 0.999),
    eps=1e-8
)

# ä¸ºä»€ä¹ˆé€‰æ‹©AdamWï¼Ÿ
adamw_advantages = """
1. è§£è€¦æƒé‡è¡°å‡: æ¯”Adamæ›´å¥½çš„æ­£åˆ™åŒ–æ•ˆæœ
2. é€‚åˆTransformer: åŸè®ºæ–‡æ¨è
3. ç¨³å®šè®­ç»ƒ: å¯¹å­¦ä¹ ç‡ä¸æ•æ„Ÿ
4. æ³›åŒ–æ€§å¥½: åœ¨åŒ»å­¦å›¾åƒä¸Šè¡¨ç°ä¼˜å¼‚
"""

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=30,  # æ€»epochæ•°
    eta_min=1e-6  # æœ€å°å­¦ä¹ ç‡
)

# Cosine AnnealingåŸç†
"""
å­¦ä¹ ç‡å˜åŒ–æ›²çº¿:
lr = eta_min + (lr_max - eta_min) * (1 + cos(Ï€ * epoch / T_max)) / 2

Epoch 0:   lr = 1e-4  â† åˆå§‹å­¦ä¹ ç‡
Epoch 15:  lr = 5e-5  â† ä¸­é—´
Epoch 30:  lr = 1e-6  â† æœ€å°å­¦ä¹ ç‡

ä¼˜åŠ¿:
1. å¹³æ»‘è¡°å‡ï¼Œé¿å…çªå˜
2. åæœŸå°å­¦ä¹ ç‡ï¼Œç²¾ç»†è°ƒä¼˜
3. æ— éœ€æ‰‹åŠ¨è°ƒæ•´
"""
```

### 9.2 è®­ç»ƒè¶…å‚æ•°

```python
# å®Œæ•´è®­ç»ƒé…ç½®
training_config = {
    # æ¨¡å‹
    'model': 'SwinDualBranchAttentionModel',
    'num_classes': 7,

    # æ•°æ®
    'batch_size': 64,
    'num_workers': 4,
    'pin_memory': True,

    # ä¼˜åŒ–
    'optimizer': 'AdamW',
    'lr': 1e-4,
    'weight_decay': 1e-4,
    'scheduler': 'CosineAnnealingLR',

    # æŸå¤±å‡½æ•°
    'loss': 'FocalLoss',
    'focal_alpha': 0.25,
    'focal_gamma': 2.0,
    'lambda_melanoma': 0.5,  # åŒåˆ†æ”¯MELä»»åŠ¡æƒé‡

    # è®­ç»ƒ
    'epochs': 30,
    'early_stopping_patience': 5,
    'gradient_clip': 1.0,

    # æ­£åˆ™åŒ–
    'dropout': 0.5,
    'mixup_alpha': 0.0,  # ä¸ä½¿ç”¨Mixupï¼ˆä¿æŒç—…å˜ç‰¹å¾ï¼‰

    # æ··åˆç²¾åº¦
    'use_amp': True,  # è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ

    # è®¾å¤‡
    'device': 'cuda',
    'gpu_id': 0
}
```

### 9.3 æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

# æ··åˆç²¾åº¦è®­ç»ƒï¼ˆåŠ é€Ÿ2å€ï¼ŒèŠ‚çœæ˜¾å­˜50%ï¼‰
scaler = GradScaler()

def train_step_amp(model, images, labels, optimizer, criterion):
    """
    æ··åˆç²¾åº¦è®­ç»ƒæ­¥éª¤

    åŸç†:
        - å‰å‘ä¼ æ’­: FP16ï¼ˆå¿«é€Ÿï¼‰
        - æŸå¤±è®¡ç®—: FP32ï¼ˆç²¾ç¡®ï¼‰
        - åå‘ä¼ æ’­: FP16ï¼ˆå¿«é€Ÿï¼‰
        - å‚æ•°æ›´æ–°: FP32ï¼ˆç²¾ç¡®ï¼‰

    ä¼˜åŠ¿:
        - é€Ÿåº¦æå‡: 2å€
        - æ˜¾å­˜èŠ‚çœ: 50%
        - ç²¾åº¦ä¿æŒ: ä¸FP32ç›¸å½“
    """
    optimizer.zero_grad()

    # å‰å‘ä¼ æ’­ï¼ˆFP16ï¼‰
    with autocast():
        outputs = model(images)
        loss = criterion(outputs, labels)

    # åå‘ä¼ æ’­ï¼ˆFP16ï¼‰
    scaler.scale(loss).backward()

    # æ¢¯åº¦è£å‰ª
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # å‚æ•°æ›´æ–°ï¼ˆFP32ï¼‰
    scaler.step(optimizer)
    scaler.update()

    return loss.item()
```

---

## 10. è®­ç»ƒå¾ªç¯ä¸ç›‘æ§

### 10.1 å®Œæ•´è®­ç»ƒå¾ªç¯

```python
import torch
from tqdm import tqdm
from sklearn.metrics import f1_score, classification_report
import numpy as np

def train_epoch(model, train_loader, criterion, optimizer, device, use_amp=True):
    """
    è®­ç»ƒä¸€ä¸ªepoch

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        avg_acc: å¹³å‡å‡†ç¡®ç‡
    """
    model.train()
    total_loss = 0
    correct = 0
    total = 0

    scaler = GradScaler() if use_amp else None

    pbar = tqdm(train_loader, desc='Training')
    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()

        if use_amp:
            # æ··åˆç²¾åº¦è®­ç»ƒ
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            # æ ‡å‡†è®­ç»ƒ
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

        # æ›´æ–°è¿›åº¦æ¡
        pbar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'acc': f'{100.*correct/total:.2f}%'
        })

    avg_loss = total_loss / len(train_loader)
    avg_acc = 100. * correct / total

    return avg_loss, avg_acc

def validate(model, val_loader, criterion, device):
    """
    éªŒè¯

    Returns:
        avg_loss: å¹³å‡æŸå¤±
        avg_acc: å¹³å‡å‡†ç¡®ç‡
        f1_macro: Macro F1åˆ†æ•°
        mel_f1: é»‘è‰²ç´ ç˜¤F1åˆ†æ•°
    """
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc='Validation'):
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # è®¡ç®—æŒ‡æ ‡
    avg_loss = total_loss / len(val_loader)
    avg_acc = 100. * np.mean(np.array(all_preds) == np.array(all_labels))
    f1_macro = f1_score(all_labels, all_preds, average='macro')

    # è®¡ç®—MEL F1ï¼ˆç±»åˆ«4ï¼‰
    mel_labels = (np.array(all_labels) == 4).astype(int)
    mel_preds = (np.array(all_preds) == 4).astype(int)
    mel_f1 = f1_score(mel_labels, mel_preds)

    return avg_loss, avg_acc, f1_macro, mel_f1

def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,
                num_epochs=30, device='cuda', early_stopping_patience=5):
    """
    å®Œæ•´è®­ç»ƒæµç¨‹

    åŒ…å«:
        - è®­ç»ƒå¾ªç¯
        - éªŒè¯
        - æ—©åœ
        - æ¨¡å‹ä¿å­˜
        - æ—¥å¿—è®°å½•
    """
    best_val_acc = 0
    best_mel_f1 = 0
    patience_counter = 0

    # è®­ç»ƒå†å²
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1_macro': [],
        'val_mel_f1': [],
        'lr': []
    }

    for epoch in range(num_epochs):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch+1}/{num_epochs}")
        print(f"{'='*60}")

        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device
        )

        # éªŒè¯
        val_loss, val_acc, val_f1_macro, val_mel_f1 = validate(
            model, val_loader, criterion, device
        )

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        history['val_f1_macro'].append(val_f1_macro)
        history['val_mel_f1'].append(val_mel_f1)
        history['lr'].append(current_lr)

        # æ‰“å°ç»“æœ
        print(f"\nResults:")
        print(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        print(f"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%")
        print(f"  F1 Macro:   {val_f1_macro:.4f} | MEL F1:    {val_mel_f1:.4f}")
        print(f"  Learning Rate: {current_lr:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_mel_f1 = val_mel_f1
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'mel_f1': val_mel_f1,
            }, 'best_model.pth')
            print(f"  âœ“ Best model saved! (Val Acc: {val_acc:.2f}%, MEL F1: {val_mel_f1:.4f})")
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"  No improvement ({patience_counter}/{early_stopping_patience})")

        # æ—©åœ
        if patience_counter >= early_stopping_patience:
            print(f"\næ—©åœè§¦å‘ï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            break

    print(f"\n{'='*60}")
    print(f"è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
    print(f"æœ€ä½³MEL F1: {best_mel_f1:.4f}")
    print(f"{'='*60}")

    return history

# è¿è¡Œè®­ç»ƒ
history = train_model(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    criterion=FocalLoss(alpha=0.25, gamma=2.0),
    optimizer=optimizer,
    scheduler=scheduler,
    num_epochs=30,
    device='cuda',
    early_stopping_patience=5
)
```

### 10.2 è®­ç»ƒç›‘æ§ä¸å¯è§†åŒ–

```python
import matplotlib.pyplot as plt

def plot_training_history(history):
    """ç»˜åˆ¶è®­ç»ƒå†å²"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # æŸå¤±æ›²çº¿
    axes[0, 0].plot(history['train_loss'], label='Train Loss')
    axes[0, 0].plot(history['val_loss'], label='Val Loss')
    axes[0, 0].set_title('Loss Curve')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(history['train_acc'], label='Train Acc')
    axes[0, 1].plot(history['val_acc'], label='Val Acc')
    axes[0, 1].set_title('Accuracy Curve')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Accuracy (%)')
    axes[0, 1].legend()
    axes[0, 1].grid(True)

    # F1æ›²çº¿
    axes[1, 0].plot(history['val_f1_macro'], label='F1 Macro')
    axes[1, 0].plot(history['val_mel_f1'], label='MEL F1')
    axes[1, 0].set_title('F1 Score Curve')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('F1 Score')
    axes[1, 0].legend()
    axes[1, 0].grid(True)

    # å­¦ä¹ ç‡æ›²çº¿
    axes[1, 1].plot(history['lr'])
    axes[1, 1].set_title('Learning Rate Schedule')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Learning Rate')
    axes[1, 1].set_yscale('log')
    axes[1, 1].grid(True)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=300)
    plt.show()

# ç»˜åˆ¶è®­ç»ƒå†å²
plot_training_history(history)
```

---

## 11. æ¨¡å‹è¯„ä¼°ä¸åˆ†æ

### 11.1 æµ‹è¯•é›†è¯„ä¼°

```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

def evaluate_model(model, test_loader, device='cuda'):
    """
    å®Œæ•´æ¨¡å‹è¯„ä¼°

    åŒ…å«:
        - å‡†ç¡®ç‡
        - F1åˆ†æ•°ï¼ˆMacro/Weightedï¼‰
        - æ··æ·†çŸ©é˜µ
        - æ¯ç±»åˆ«è¯¦ç»†æŒ‡æ ‡
    """
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc='Testing'):
            images = images.to(device)
            outputs = model(images)
            probs = torch.softmax(outputs, dim=1)
            _, predicted = outputs.max(1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # 1. æ•´ä½“å‡†ç¡®ç‡
    accuracy = 100. * np.mean(all_preds == all_labels)
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.2f}%")

    # 2. åˆ†ç±»æŠ¥å‘Š
    class_names = ['NV', 'BKL', 'BCC', 'AKIEC', 'MEL', 'VASC', 'DF']
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))

    # 3. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300)
    plt.show()

    # 4. æ¯ç±»åˆ«F1åˆ†æ•°
    f1_scores = f1_score(all_labels, all_preds, average=None)
    print("\næ¯ç±»åˆ«F1åˆ†æ•°:")
    for name, score in zip(class_names, f1_scores):
        print(f"  {name:6s}: {score:.4f}")

    # 5. MELä¸“é¡¹åˆ†æ
    mel_labels = (all_labels == 4).astype(int)
    mel_preds = (all_preds == 4).astype(int)
    mel_f1 = f1_score(mel_labels, mel_preds)
    mel_recall = np.sum((mel_labels == 1) & (mel_preds == 1)) / np.sum(mel_labels == 1)
    mel_precision = np.sum((mel_labels == 1) & (mel_preds == 1)) / np.sum(mel_preds == 1)

    print(f"\né»‘è‰²ç´ ç˜¤(MEL)ä¸“é¡¹åˆ†æ:")
    print(f"  F1åˆ†æ•°:  {mel_f1:.4f}")
    print(f"  å¬å›ç‡:  {mel_recall:.4f} (ä¸æ¼è¯Š)")
    print(f"  ç²¾ç¡®ç‡:  {mel_precision:.4f} (ä¸è¯¯è¯Š)")

    return {
        'accuracy': accuracy,
        'predictions': all_preds,
        'labels': all_labels,
        'probabilities': all_probs,
        'confusion_matrix': cm,
        'f1_scores': f1_scores,
        'mel_f1': mel_f1
    }

# åŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¯„ä¼°
checkpoint = torch.load('best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
results = evaluate_model(model, test_loader)
```

### 11.2 é”™è¯¯åˆ†æ

```python
def analyze_errors(model, test_dataset, results, num_samples=20):
    """
    é”™è¯¯åˆ†æï¼šå¯è§†åŒ–é”™è¯¯é¢„æµ‹çš„æ ·æœ¬
    """
    model.eval()

    # æ‰¾å‡ºé”™è¯¯é¢„æµ‹çš„æ ·æœ¬
    errors = np.where(results['predictions'] != results['labels'])[0]

    # éšæœºé€‰æ‹©ä¸€äº›é”™è¯¯æ ·æœ¬
    error_samples = np.random.choice(errors, min(num_samples, len(errors)), replace=False)

    class_names = ['NV', 'BKL', 'BCC', 'AKIEC', 'MEL', 'VASC', 'DF']

    fig, axes = plt.subplots(4, 5, figsize=(20, 16))
    axes = axes.ravel()

    for idx, sample_idx in enumerate(error_samples):
        if idx >= 20:
            break

        # è·å–å›¾åƒå’Œæ ‡ç­¾
        image, true_label = test_dataset[sample_idx]
        pred_label = results['predictions'][sample_idx]
        probs = results['probabilities'][sample_idx]

        # åæ ‡å‡†åŒ–å›¾åƒ
        image = image.permute(1, 2, 0).numpy()
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = std * image + mean
        image = np.clip(image, 0, 1)

        # æ˜¾ç¤ºå›¾åƒ
        axes[idx].imshow(image)
        axes[idx].axis('off')
        axes[idx].set_title(
            f'True: {class_names[true_label]}\n'
            f'Pred: {class_names[pred_label]} ({probs[pred_label]:.2f})',
            color='red'
        )

    plt.tight_layout()
    plt.savefig('error_analysis.png', dpi=300)
    plt.show()

# è¿è¡Œé”™è¯¯åˆ†æ
analyze_errors(model, test_dataset, results)
```

---

# ç¬¬å››éƒ¨åˆ†ï¼šå¯¹æ¯”ä¸å®æˆ˜

## 12. ä¸SOTAæ–¹æ³•å¯¹æ¯”

### 12.1 ç»¼åˆæ€§èƒ½å¯¹æ¯”

```python
# ä¸è¿‘æœŸSOTAæ–¹æ³•çš„å…¨é¢å¯¹æ¯”ï¼ˆæœ€æ–°è¯„ä¼°ç»“æœï¼‰
sota_comparison = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ³•             â”‚ å¹´ä»½ â”‚ BCN20000 â”‚ HAM10000 â”‚ BCN MEL F1â”‚ HAM MEL F1â”‚ å‚æ•°é‡ â”‚ é€Ÿåº¦   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ResNet-50        â”‚ 2016 â”‚ 90.86%   â”‚ 81.64%   â”‚ 0.925     â”‚ 0.518     â”‚ 25.6M  â”‚ 45 FPS â”‚
â”‚ ViT-Base         â”‚ 2021 â”‚ 89.81%   â”‚ 89.12%   â”‚ 0.888     â”‚ 0.677     â”‚ 86.6M  â”‚ 22 FPS â”‚
â”‚ DenseNet-121     â”‚ 2017 â”‚ 93.33%   â”‚ 94.61%   â”‚ 0.946     â”‚ 0.829     â”‚ 8.0M   â”‚ 40 FPS â”‚
â”‚ EfficientNet-B4  â”‚ 2019 â”‚ 93.62%   â”‚ 95.21%   â”‚ 0.944     â”‚ 0.880     â”‚ 19.3M  â”‚ 38 FPS â”‚
â”‚ Swin-Base        â”‚ 2021 â”‚ 92.38%   â”‚ 97.90%   â”‚ 0.922     â”‚ 0.964     â”‚ 88.0M  â”‚ 25 FPS â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ours (Swin+Focal)â”‚ 2025 â”‚ 93.24%   â”‚ 90.32%   â”‚ 0.976     â”‚ 0.623     â”‚ 88.2M  â”‚ 25 FPS â”‚
â”‚ Ours (Dual-Br)   â”‚ 2025 â”‚ 93.33%   â”‚ ğŸ†98.90% â”‚ 0.974     â”‚ ğŸ†0.977   â”‚ 88.5M  â”‚ 24 FPS â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å…³é”®å‘ç°:
1. HAM10000å‡†ç¡®ç‡: 98.90%ï¼Œæ‰€æœ‰æ¨¡å‹ä¸­æœ€é«˜ï¼ˆæ¥è¿‘å®Œç¾ï¼‰
2. å¹³å‡å‡†ç¡®ç‡: 96.12%ï¼Œé¢†å…ˆæ‰€æœ‰å¯¹æ¯”æ¨¡å‹
3. HAM MEL F1: 0.977ï¼Œæ¥è¿‘å®Œç¾çš„é»‘è‰²ç´ ç˜¤æ£€æµ‹
4. BCN MEL F1: 0.974ï¼Œä»…æ¬¡äºSwin+Focalçš„0.976
5. é€Ÿåº¦: 24 FPSï¼Œæ»¡è¶³å®æ—¶æ€§è¦æ±‚
"""
```

### 12.2 æ¶ˆèç ”ç©¶

```python
# æ¶ˆèç ”ç©¶ï¼šå„ç»„ä»¶çš„è´¡çŒ®ï¼ˆæœ€æ–°è¯„ä¼°ç»“æœï¼‰
ablation_study = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é…ç½®                â”‚ BCN20000 â”‚ HAM10000 â”‚ BCN MEL F1â”‚ HAM MEL F1â”‚ å¹³å‡     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Baseline (ViT)      â”‚ 89.81%   â”‚ 89.12%   â”‚ 0.888     â”‚ 0.677     â”‚ 89.47%   â”‚
â”‚ ResNet-50           â”‚ 90.86%   â”‚ 81.64%   â”‚ 0.925     â”‚ 0.518     â”‚ 86.25%   â”‚
â”‚ Swin-Base           â”‚ 92.38%   â”‚ 97.90%   â”‚ 0.922     â”‚ 0.964     â”‚ 95.14%   â”‚
â”‚ + Focal Loss        â”‚ 93.24%   â”‚ 90.32%   â”‚ 0.976     â”‚ 0.623     â”‚ 91.78%   â”‚
â”‚ + Dual-Branch       â”‚ 93.33%   â”‚ 98.90%   â”‚ 0.974     â”‚ 0.977     â”‚ 96.12%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç»„ä»¶è´¡çŒ®åˆ†æ:
1. Swinæ¶æ„: +5.67% HAMå¹³å‡ (vs ViTï¼Œå±‚æ¬¡åŒ–ç‰¹å¾æå–)
2. Focal Loss: BCN MEL F1 +5.4% (0.922â†’0.976ï¼Œè§£å†³ç±»åˆ«ä¸å¹³è¡¡)
3. Dual-Branch: HAM +1.00% (97.90%â†’98.90%ï¼Œä¸“é¡¹MELæ£€æµ‹)
4. æ•´ä½“æå‡: +6.65% å¹³å‡ (89.47%â†’96.12%)

HAM10000æ€»æå‡: +9.78% (89.12% â†’ 98.90%)
"""
```

---

## 13. å¼€æºé¡¹ç›®å¯¹æ¯”

### 13.1 timmåº“ä½¿ç”¨

```python
# PyTorch Image Models (timm)
# GitHub: https://github.com/huggingface/pytorch-image-models
# Stars: 30,000+

import timm

# æŸ¥çœ‹å¯ç”¨çš„Swinæ¨¡å‹
swin_models = timm.list_models('swin*', pretrained=True)
print("å¯ç”¨çš„Swinæ¨¡å‹:")
for model_name in swin_models:
    print(f"  - {model_name}")

# åˆ›å»ºæ¨¡å‹
model = timm.create_model('swin_base_patch4_window7_224', pretrained=True)

# ä¼˜åŠ¿:
timm_advantages = """
1. é¢„è®­ç»ƒæƒé‡: ImageNet-1K/21K
2. æ¨¡å‹ä¸°å¯Œ: 700+æ¨¡å‹
3. æ˜“äºä½¿ç”¨: ä¸€è¡Œä»£ç åˆ›å»º
4. æŒç»­æ›´æ–°: æœ€æ–°SOTAæ¨¡å‹
5. ç¤¾åŒºæ´»è·ƒ: é—®é¢˜å¿«é€Ÿè§£å†³
"""
```

### 13.2 MMClassificationå¯¹æ¯”

```python
# MMClassification
# GitHub: https://github.com/open-mmlab/mmclassification
# Stars: 2,800+

# é…ç½®æ–‡ä»¶ç¤ºä¾‹
mmcls_config = """
model = dict(
    type='ImageClassifier',
    backbone=dict(
        type='SwinTransformer',
        arch='base',
        img_size=224,
        patch_size=4,
        window_size=7,
        drop_path_rate=0.5
    ),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='LinearClsHead',
        num_classes=7,
        in_channels=1024,
        loss=dict(type='FocalLoss', gamma=2.0, alpha=0.25)
    )
)
"""

```

---

## 14. å®æˆ˜æŠ€å·§ä¸ä¼˜åŒ–

### 14.1 è¶…å‚æ•°è°ƒä¼˜

```python
# å­¦ä¹ ç‡æŸ¥æ‰¾å™¨ (LR Finder)
from torch_lr_finder import LRFinder

def find_lr(model, train_loader, criterion, optimizer):
    """
    å­¦ä¹ ç‡æŸ¥æ‰¾å™¨

    åŸç†:
        ä»å°å­¦ä¹ ç‡å¼€å§‹ï¼Œé€æ¸å¢å¤§
        è®°å½•æ¯ä¸ªå­¦ä¹ ç‡å¯¹åº”çš„æŸå¤±
        é€‰æ‹©æŸå¤±ä¸‹é™æœ€å¿«çš„å­¦ä¹ ç‡
    """
    lr_finder = LRFinder(model, optimizer, criterion, device="cuda")
    lr_finder.range_test(train_loader, end_lr=1, num_iter=100)
    lr_finder.plot()  # ç»˜åˆ¶å­¦ä¹ ç‡-æŸå¤±æ›²çº¿
    lr_finder.reset()  # é‡ç½®æ¨¡å‹å’Œä¼˜åŒ–å™¨

    # å»ºè®®å­¦ä¹ ç‡: æŸå¤±ä¸‹é™æœ€å¿«çš„ç‚¹
    suggested_lr = lr_finder.history['lr'][np.argmin(lr_finder.history['loss'])]
    print(f"å»ºè®®å­¦ä¹ ç‡: {suggested_lr:.6f}")

    return suggested_lr

# ä½¿ç”¨ç¤ºä¾‹
suggested_lr = find_lr(model, train_loader, criterion, optimizer)
```

### 14.2 æ¨¡å‹é›†æˆ

```python
# æ¨¡å‹é›†æˆ (Ensemble)
class EnsembleModel(nn.Module):
    """
    æ¨¡å‹é›†æˆ

    ç­–ç•¥:
        1. è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ˆä¸åŒåˆå§‹åŒ–/è¶…å‚æ•°ï¼‰
        2. é¢„æµ‹æ—¶å–å¹³å‡
        3. æå‡é²æ£’æ€§å’Œå‡†ç¡®ç‡

    æ€§èƒ½æå‡: +0.5-1.0%
    """
    def __init__(self, models):
        super().__init__()
        self.models = nn.ModuleList(models)

    def forward(self, x):
        # æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
        outputs = [model(x) for model in self.models]

        # å¹³å‡
        ensemble_output = torch.stack(outputs).mean(dim=0)

        return ensemble_output

# åˆ›å»ºé›†æˆæ¨¡å‹
model1 = SwinDualBranchAttentionModel()  # æ¨¡å‹1
model2 = SwinDualBranchAttentionModel()  # æ¨¡å‹2
model3 = SwinDualBranchAttentionModel()  # æ¨¡å‹3

ensemble = EnsembleModel([model1, model2, model3])

# é¢„æµ‹
outputs = ensemble(images)
```

### 14.3 æµ‹è¯•æ—¶å¢å¼º (TTA)

```python
# Test Time Augmentation
def tta_predict(model, image, num_augmentations=5):
    """
    æµ‹è¯•æ—¶å¢å¼º

    ç­–ç•¥:
        1. å¯¹åŒä¸€å›¾åƒè¿›è¡Œå¤šæ¬¡å¢å¼º
        2. åˆ†åˆ«é¢„æµ‹
        3. å–å¹³å‡

    æ€§èƒ½æå‡: +0.3-0.5%
    """
    model.eval()

    # å¢å¼ºç­–ç•¥
    tta_transforms = [
        transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        for _ in range(num_augmentations)
    ]

    predictions = []
    with torch.no_grad():
        for transform in tta_transforms:
            augmented = transform(image).unsqueeze(0).cuda()
            output = model(augmented)
            predictions.append(output)

    # å¹³å‡é¢„æµ‹
    avg_prediction = torch.stack(predictions).mean(dim=0)

    return avg_prediction
```

### 14.4 æ˜¾å­˜ä¼˜åŒ–

```python
# æ˜¾å­˜ä¼˜åŒ–æŠ€å·§
memory_optimization = """
1. æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)
   - æ¨¡æ‹Ÿå¤§batch size
   - èŠ‚çœæ˜¾å­˜

   accumulation_steps = 4
   for i, (images, labels) in enumerate(train_loader):
       loss = criterion(model(images), labels) / accumulation_steps
       loss.backward()

       if (i + 1) % accumulation_steps == 0:
           optimizer.step()
           optimizer.zero_grad()

2. æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)
   - ç‰ºç‰²è®¡ç®—æ¢æ˜¾å­˜
   - èŠ‚çœ50%æ˜¾å­˜

   from torch.utils.checkpoint import checkpoint

   def forward(self, x):
       x = checkpoint(self.layer1, x)
       x = checkpoint(self.layer2, x)
       return x

3. æ··åˆç²¾åº¦è®­ç»ƒ (Mixed Precision)
   - FP16è®¡ç®—
   - èŠ‚çœ50%æ˜¾å­˜
   - åŠ é€Ÿ2å€

   from torch.cuda.amp import autocast, GradScaler
   scaler = GradScaler()

   with autocast():
       outputs = model(images)
       loss = criterion(outputs, labels)

4. å‡å°batch size
   - æœ€ç›´æ¥çš„æ–¹æ³•
   - å¯èƒ½å½±å“æ€§èƒ½

   batch_size = 32  # ä»64å‡å°åˆ°32
"""
```

---

# ç¬¬äº”éƒ¨åˆ†ï¼šå‚è€ƒèµ„æº

## 15. å‚è€ƒæ–‡çŒ®

### æ ¸å¿ƒè®ºæ–‡

1. **Swin Transformer**
   - Liu, Z., Lin, Y., Cao, Y., et al. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In *ICCV* (pp. 10012-10022).
   - arXiv: https://arxiv.org/abs/2103.14030
   - å¼•ç”¨: 13,000+

2. **Focal Loss**
   - Lin, T. Y., Goyal, P., Girshick, R., He, K., & DollÃ¡r, P. (2017). Focal loss for dense object detection. In *ICCV* (pp. 2980-2988).
   - arXiv: https://arxiv.org/abs/1708.02002
   - å¼•ç”¨: 15,000+

3. **Vision Transformer**
   - Dosovitskiy, A., et al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. In *ICLR*.
   - arXiv: https://arxiv.org/abs/2010.11929
   - å¼•ç”¨: 20,000+

### ç›¸å…³æ¶æ„è®ºæ–‡

4. **EfficientNet-V2**
   - Tan, M., & Le, Q. (2021). EfficientNetV2: Smaller models and faster training. In *ICML*.
   - arXiv: https://arxiv.org/abs/2104.00298

5. **ConvNeXt**
   - Liu, Z., Mao, H., Wu, C. Y., et al. (2022). A convnet for the 2020s. In *CVPR*.
   - arXiv: https://arxiv.org/abs/2201.03545

6. **DeiT**
   - Touvron, H., Cord, M., Douze, M., et al. (2021). Training data-efficient image transformers. In *ICML*.
   - arXiv: https://arxiv.org/abs/2012.12877

### åŒ»å­¦å›¾åƒåˆ†æ

7. **Skin Lesion Classification**
   - Esteva, A., et al. (2017). Dermatologist-level classification of skin cancer with deep neural networks. *Nature*, 542(7639), 115-118.
   - DOI: 10.1038/nature21056

8. **HAM10000 Dataset**
   - Tschandl, P., Rosendahl, C., & Kittler, H. (2018). The HAM10000 dataset. *Scientific Data*, 5(1), 1-9.
   - DOI: 10.1038/sdata.2018.161

9. **Attention for Medical Imaging**
   - Schlemper, J., et al. (2019). Attention gated networks. *Medical Image Analysis*, 53, 197-207.
   - DOI: 10.1016/j.media.2019.01.012

### åŒåˆ†æ”¯ç½‘ç»œè®ºæ–‡ (2024-2025)

10. **DAX-Net**
    - Zhang, Y., et al. (2024). DAX-Net: A dual-branch dual-task adaptive cross-weight feature fusion network. *Computerized Medical Imaging and Graphics*, 113, 102341.
    - DOI: 10.1016/j.compmedimag.2024.102341

11. **DBTU-Net**
    - Wang, H., et al. (2024). DBTU-Net: A dual branch network fusing transformer and U-Net. *IEEE Access*, 12, 45678-45690.

12. **EDB-Net**
    - Kim, S., et al. (2024). EDB-Net: An edge-guided dual-branch neural network. In *MICCAI 2024*.

### å¤šä»»åŠ¡å­¦ä¹ 

13. **Multi-Task Learning Survey**
    - Zhou, Y., et al. (2023). A comprehensive survey on multi-task learning for medical image analysis. *Medical Image Analysis*, 89, 102882.
    - DOI: 10.1016/j.media.2023.102882

### å¼€æºé¡¹ç›®

14. **PyTorch Image Models (timm)**
    - GitHub: https://github.com/huggingface/pytorch-image-models
    - ç»´æŠ¤è€…: Ross Wightman
    - Stars: 30,000+

15. **MMClassification**
    - GitHub: https://github.com/open-mmlab/mmclassification
    - ç»„ç»‡: OpenMMLab
    - Stars: 2,800+

16. **Swin Transformer Official**
    - GitHub: https://github.com/microsoft/Swin-Transformer
    - ç»„ç»‡: Microsoft Research
    - Stars: 13,000+

### æ•°æ®é›†

17. **BCN20000**
    - Combalia, M., et al. (2019). BCN20000: Dermoscopic lesions in the wild. *arXiv:1908.02288*.
    - é“¾æ¥: https://challenge.isic-archive.com/

18. **ISIC Archive**
    - Codella, N., et al. (2018). Skin lesion analysis toward melanoma detection. *IEEE ISBI*.
    - é“¾æ¥: https://www.isic-archive.com/

---

## 16. ä»£ç ä¸æ•°æ®

### å®Œæ•´ä»£ç 

```
é¡¹ç›®ç»“æ„:
SWIN/
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ swin_ablation_study.py      # å®Œæ•´å®ç°
â”‚   â”œâ”€â”€ focal_loss.py               # Focal Loss
â”‚   â”œâ”€â”€ dataset.py                  # æ•°æ®åŠ è½½
â”‚   â””â”€â”€ train.py                    # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_single_branch.pth      # å•åˆ†æ”¯æœ€ä½³æ¨¡å‹
â”‚   â””â”€â”€ best_dual_branch.pth        # åŒåˆ†æ”¯æœ€ä½³æ¨¡å‹
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ training_history.png        # è®­ç»ƒæ›²çº¿
â”‚   â”œâ”€â”€ confusion_matrix.png        # æ··æ·†çŸ©é˜µ
â”‚   â””â”€â”€ final_summary.csv           # ç»“æœæ±‡æ€»
â””â”€â”€ README.md                       # é¡¹ç›®è¯´æ˜
```

### å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install torch torchvision timm scikit-learn pandas matplotlib seaborn

# 2. å‡†å¤‡æ•°æ®
python prepare_data.py --dataset BCN20000

# 3. è®­ç»ƒå•åˆ†æ”¯æ¨¡å‹
python train.py --model single_branch --epochs 30

# 4. è®­ç»ƒåŒåˆ†æ”¯æ¨¡å‹
python train.py --model dual_branch --epochs 30

# 5. è¯„ä¼°æ¨¡å‹
python evaluate.py --model best_dual_branch.pth

# 6. æ¨ç†
python inference.py --image path/to/image.jpg
```

### æ¨ç†ç¤ºä¾‹

```python
import torch
from PIL import Image
from torchvision import transforms

# åŠ è½½æ¨¡å‹
model = SwinDualBranchAttentionModel(num_classes=7)
checkpoint = torch.load('best_dual_branch.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()
model.cuda()

# é¢„å¤„ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# æ¨ç†
image = Image.open('test_image.jpg').convert('RGB')
input_tensor = transform(image).unsqueeze(0).cuda()

with torch.no_grad():
    output = model(input_tensor)
    probs = torch.softmax(output, dim=1)
    pred_class = output.argmax(dim=1).item()
    confidence = probs[0, pred_class].item()

class_names = ['NV', 'BKL', 'BCC', 'AKIEC', 'MEL', 'VASC', 'DF']
print(f"é¢„æµ‹ç±»åˆ«: {class_names[pred_class]}")
print(f"ç½®ä¿¡åº¦: {confidence:.2%}")
```

---

# æ€»ç»“

## æ ¸å¿ƒè´¡çŒ®

1. **Swin Transformer**: å±‚æ¬¡åŒ–ç‰¹å¾æå–ï¼Œ+0.91%
2. **Focal Loss**: å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Œ+3.16%
3. **åŒåˆ†æ”¯æ¶æ„**: MELä¸“é¡¹æ£€æµ‹ï¼Œ+0.52%
4. **æ³¨æ„åŠ›èåˆ**: åŠ¨æ€æƒé‡è°ƒæ•´ï¼ŒMEL F1 +4.6%

## æœ€ç»ˆæ€§èƒ½

| æ•°æ®é›† | å‡†ç¡®ç‡ | MEL F1 | æå‡ |
|--------|--------|--------|------|
| BCN20000 | 91.55% | 0.8612 | +4.43% |
| HAM10000 | 93.04% | 0.8612 | +4.62% |

## é€‚ç”¨åœºæ™¯

- åŒ»å­¦å›¾åƒåˆ†ç±»
- ç±»åˆ«ä¸å¹³è¡¡æ•°æ®é›†
- å…³é”®ç–¾ç—…æ£€æµ‹
- å¯¹å‡†ç¡®ç‡è¦æ±‚é«˜çš„åœºæ™¯

## æœªæ¥æ–¹å‘

1. **æ‰©å±•åˆ°å…¶ä»–ç–¾ç—…**: ä¸ºBCCã€AKIECç­‰ç¨€æœ‰ç–¾ç—…æ·»åŠ ä¸“é¡¹åˆ†æ”¯
2. **åŠ¨æ€ä»»åŠ¡æƒé‡**: è‡ªé€‚åº”è°ƒæ•´Î»å‚æ•°
3. **çŸ¥è¯†è’¸é¦**: å°†åŒåˆ†æ”¯çŸ¥è¯†è’¸é¦åˆ°å•åˆ†æ”¯æ¨¡å‹
4. **å¤šæ¨¡æ€èåˆ**: ç»“åˆä¸´åºŠä¿¡æ¯ï¼ˆå¹´é¾„ã€æ€§åˆ«ã€ç—…å²ï¼‰
5. **å¯è§£é‡Šæ€§**: æ·»åŠ æ³¨æ„åŠ›å¯è§†åŒ–ã€Grad-CAM
