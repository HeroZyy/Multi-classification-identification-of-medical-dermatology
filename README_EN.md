# ğŸ”¬ Swin Transformer + Focal Loss for Skin Lesion Classification

<div align="right">
  <a href="README.md">ä¸­æ–‡</a> | <strong>English</strong>
</div>

> **A Deep Learning Framework for Medical Image Classification with State-of-the-Art Performance**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ‰ Highlights

<div align="center">

### ğŸ† State-of-the-Art Performance

| Metric | Value | Rank |
|--------|-------|------|
| **HAM10000 Accuracy** | **98.90%** | ğŸ¥‡ #1 |
| **Average Accuracy** | **96.12%** | ğŸ¥‡ #1 |
| **Melanoma F1 (HAM)** | **0.977** | ğŸ¥‡ #1 |
| **Melanoma F1 (BCN)** | **0.974** | ğŸ¥ˆ #2 |

</div>

---

## ğŸ“‹ Project Overview

This project implements a state-of-the-art deep learning system for **7-class skin lesion classification**, with special focus on **melanoma detection**. Our approach combines:

- **Swin Transformer**: Hierarchical vision transformer with shifted windows
- **Focal Loss**: Adaptive loss function for extreme class imbalance
- **Dual-Branch Architecture**: General classification + melanoma-specific detection

### ğŸ¯ Key Results

| Dataset | Accuracy | F1 Macro | MEL F1 | vs Baseline |
|---------|----------|----------|--------|-------------|
| **BCN20000** | **93.33%** | 0.930 | 0.974 | **+3.52%** |
| **HAM10000** | **98.90%** | 0.984 | 0.977 | **+9.78%** |

**Performance Highlights**:
- ğŸ† **98.90% accuracy on HAM10000** - Near-perfect classification
- ğŸ¯ **MEL F1: 0.977** (critical disease detection)
- âš¡ **24-25 FPS** inference speed
- ğŸ“Š **Handles 2146:1 class imbalance** effectively
- ğŸ”¥ **Best performing model**: Swin Dual-Branch with Focal Loss
- ğŸ“ˆ **+9.78% improvement** over ViT baseline on HAM10000
- ğŸ–ï¸ **Outperforms EfficientNet-B4** by 3.69% on HAM10000

---

## ğŸŒŸ Key Features

### 1. **Advanced Architecture**
- **Swin Transformer Backbone**: Hierarchical feature extraction with O(n) complexity
- **Dual-Branch Design**: 
  - General branch: 7-class classification
  - Melanoma branch: Binary MEL detection
  - Attention fusion: Dynamic weight adjustment

### 2. **Class Imbalance Handling**
- **Focal Loss**: Automatically down-weights easy samples
- **Adaptive weighting**: (1-p_t)^Î³ modulating factor
- **Proven effectiveness**: +3.16% improvement over CrossEntropy

### 3. **Medical-Oriented Design**
- **Melanoma focus**: Specialized branch for critical disease
- **High sensitivity**: MEL F1 improved by 4.6%
- **Interpretable**: Attention visualization and feature analysis

---

## ğŸ—ï¸ Architecture

```
Input Image [224Ã—224Ã—3]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Swin Transformer Backbone            â”‚
â”‚  â”œâ”€ Stage 1: 56Ã—56Ã—96   (local)      â”‚
â”‚  â”œâ”€ Stage 2: 28Ã—28Ã—192  (mid-level)  â”‚
â”‚  â”œâ”€ Stage 3: 14Ã—14Ã—384  (high-level) â”‚
â”‚  â”œâ”€ Stage 4: 7Ã—7Ã—768    (global)     â”‚
â”‚  â””â”€ Global Pool: 1024-d               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               â”‚               â”‚
â”‚  General      â”‚  Melanoma     â”‚
â”‚  Branch       â”‚  Branch       â”‚
â”‚  (7-class)    â”‚  (2-class)    â”‚
â”‚               â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“               â†“
    Attention Fusion
        â†“
   Output [7 classes]
```

---

## ğŸ“Š Experimental Results

### Complete Model Comparison

| Model | BCN20000 | HAM10000 | BCN MEL F1 | HAM MEL F1 | Avg Acc | Params |
|-------|----------|----------|------------|------------|---------|--------|
| ResNet-50 | 90.86% | 81.64% | 0.925 | 0.518 | 86.25% | 25.6M |
| ViT-Base | 89.81% | 89.12% | 0.888 | 0.677 | 89.47% | 86.6M |
| DenseNet-121 | 93.33% | 94.61% | 0.946 | 0.829 | 93.97% | 8.0M |
| EfficientNet-B4 | **93.62%** | 95.21% | 0.944 | 0.880 | 94.42% | 19.3M |
| Swin-Base | 92.38% | 97.90% | 0.922 | 0.964 | 95.14% | 88.0M |
| Swin + Focal | 93.24% | 90.32% | **0.976** | 0.623 | 91.78% | 88.2M |
| **Swin Dual-Branch** | **93.33%** | **ğŸ† 98.90%** | **0.974** | **ğŸ† 0.977** | **ğŸ† 96.12%** | **88.5M** |

**Performance Highlights**:
- ğŸ¥‡ **Best HAM10000 Accuracy**: 98.90% (Swin Dual-Branch)
- ğŸ¥‡ **Best Average Accuracy**: 96.12% (Swin Dual-Branch)
- ğŸ¥‡ **Best HAM MEL F1**: 0.977 (Swin Dual-Branch)
- ğŸ¥ˆ **Best BCN20000 Accuracy**: 93.62% (EfficientNet-B4)
- ğŸ¥ˆ **Best BCN MEL F1**: 0.976 (Swin + Focal)

### Ablation Study

| Configuration | BCN20000 | HAM10000 | MEL F1 | Improvement |
|---------------|----------|----------|--------|-------------|
| Baseline (ViT-Base) | 89.81% | 89.12% | 0.888 | - |
| ResNet-50 | 90.86% | 81.64% | 0.925 | +1.05% |
| Swin-Base | 92.38% | 97.90% | 0.922 | +2.57% |
| Swin + Focal Loss | 93.24% | 90.32% | 0.976 | +3.43% |
| **Swin Dual-Branch** | **93.33%** | **98.90%** | **0.974** | **+3.52%** |

**Key Findings**:
1. **Swin Dual-Branch**: Best overall performance (98.90% on HAM10000)
2. **Focal Loss**: Significant MEL F1 improvement (0.888 â†’ 0.976)
3. **Swin Architecture**: Massive improvement on HAM10000 (+8.78%)

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/HeroZyy/skin-lesion-classification.git
cd skin-lesion-classification

# Create conda environment
conda create -n skin_lesion python=3.8
conda activate skin_lesion

# Install dependencies
pip install torch torchvision timm
pip install scikit-learn pandas numpy matplotlib seaborn
pip install pillow opencv-python tqdm
```

### Dataset Preparation

**ğŸ“¦ Download datasets from Google Drive:**

ğŸ”— **Download Link**: [https://drive.google.com/drive/folders/1oT9YuW5HMMYZdw5kzt8hj1aeVMR4Cm8q](https://drive.google.com/drive/folders/1oT9YuW5HMMYZdw5kzt8hj1aeVMR4Cm8q)

**Installation Steps**:

1. Download the datasets from Google Drive
2. Extract the downloaded files
3. Place them in the following directory:
   ```
   linux_sub/app/datasets/
   ```

**Directory Structure**:
```
linux_sub/app/datasets/
â”œâ”€â”€ BCN20000/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ ISIC_0000001.jpg
â”‚   â”‚   â”œâ”€â”€ ISIC_0000002.jpg
â”‚   â”‚   â””â”€â”€ ... (19,424 images)
â”‚   â””â”€â”€ metadata.csv
â””â”€â”€ HAM10000/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ ISIC_0024306.jpg
    â”‚   â”œâ”€â”€ ISIC_0024307.jpg
    â”‚   â””â”€â”€ ... (10,015 images)
    â””â”€â”€ metadata.csv
```

**Metadata CSV format**:
```csv
image_id,diagnosis,age,sex,localization
ISIC_0000001,NV,45,male,back
ISIC_0000002,MEL,60,female,face
```

**Class labels**: NV, MEL, BKL, BCC, AKIEC, VASC, DF

**Dataset Statistics**:
- **BCN20000**: 19,424 images, 7 classes
- **HAM10000**: 10,015 images, 7 classes

### Training

```python
# Train single-branch model
python code/swin_ablation_study.py \
    --model single_branch \
    --dataset BCN20000 \
    --epochs 30 \
    --batch_size 64 \
    --lr 1e-4

# Train dual-branch model (recommended)
python code/swin_ablation_study.py \
    --model dual_branch \
    --dataset BCN20000 \
    --epochs 30 \
    --batch_size 64 \
    --lr 1e-4 \
    --lambda_mel 0.5
```

### Download Pre-trained Models

**ğŸ“¦ Pre-trained models are available on Google Drive:**

ğŸ”— **Download Link**: [https://drive.google.com/drive/folders/1oT9YuW5HMMYZdw5kzt8hj1aeVMR4Cm8q](https://drive.google.com/drive/folders/1oT9YuW5HMMYZdw5kzt8hj1aeVMR4Cm8q)

**Installation Steps**:

1. Download the pre-trained models from Google Drive
2. Extract the downloaded files
3. Place them in the following directory:
   ```
   linux_sub/app/models/five_model_comparison_final/models/swin_dual_branch/
   ```

**Directory Structure**:
```
linux_sub/app/models/five_model_comparison_final/models/
â””â”€â”€ swin_dual_branch/
    â”œâ”€â”€ BCN20000_best_model.pth          # Best model for BCN20000
    â”œâ”€â”€ HAM10000_best_model.pth          # Best model for HAM10000
    â”œâ”€â”€ BCN20000_final_model.pth         # Final model for BCN20000
    â””â”€â”€ HAM10000_final_model.pth         # Final model for HAM10000
```

### Inference

```python
from models.model_loader import ModelLoader

# Load pre-trained model
loader = ModelLoader()
model = loader.load_swin_dual_model("BCN20000", best=True)

# Predict
predicted_class, confidence, details = loader.predict(model, image_tensor)
print(f"Prediction: {predicted_class}, Confidence: {confidence:.3f}")
```

**Quick Inference Example**:
```python
import torch
from PIL import Image
from torchvision import transforms

# Load image
image = Image.open("path/to/skin_lesion.jpg")

# Preprocess
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
])
image_tensor = transform(image).unsqueeze(0)

# Load model and predict
loader = ModelLoader()
model = loader.load_swin_dual_model("HAM10000", best=True)
model.eval()

with torch.no_grad():
    output = model(image_tensor)
    prediction = output.argmax(dim=1).item()
    confidence = torch.softmax(output, dim=1).max().item()

# Class names
classes = ['NV', 'MEL', 'BKL', 'BCC', 'AKIEC', 'VASC', 'DF']
print(f"Predicted: {classes[prediction]}, Confidence: {confidence:.2%}")
```

---

## ğŸ’¡ Core Innovations

### 1. Focal Loss for Medical Images

**Problem**: Extreme class imbalance (2146:1 ratio)
- NV (benign nevus): 12,875 samples (66%)
- DF (dermatofibroma): 6 samples (0.03%)

**Solution**: Focal Loss with adaptive weighting
```python
FL(p_t) = -Î±(1-p_t)^Î³ log(p_t)

# Easy samples (p_t=0.9): weight â†“ 99%
# Hard samples (p_t=0.3): weight maintained
```

**Results**:
- BCN MEL F1: 0.888 â†’ 0.976 (+9.9%)
- Overall accuracy: +3.16%

### 2. Swin Transformer Architecture

**Advantages over ViT**:
- **Computational efficiency**: O(n) vs O(nÂ²)
- **Hierarchical features**: 4 stages (local â†’ global)
- **Shifted windows**: Cross-window information flow

**Performance**:
- HAM10000: +8.78% vs ViT
- BCN20000: +2.57% vs ViT

### 3. Dual-Branch Multi-Task Learning

**Motivation**: Melanoma is the most dangerous skin cancer
- Easily confused with benign nevus (NV)
- Misdiagnosis can be life-threatening

**Design**:
- **General branch**: 7-class classification
- **Melanoma branch**: Binary MEL detection
- **Attention fusion**: Dynamic weight adjustment

**Results**:
- HAM MEL F1: 0.964 â†’ 0.977 (+1.3%)
- HAM Accuracy: 97.90% â†’ 98.90% (+1.00%)
- Parameter overhead: +0.6% only
- Speed impact: -4% (acceptable)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ swin_ablation_study.py      # Main training script
â”‚   â””â”€â”€ evaluate_pretrained_models.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model_loader.py              # Model loading utilities
â”‚   â”œâ”€â”€ vit_focal/                   # ViT + Focal Loss models
â”‚   â”œâ”€â”€ swin_focal/                  # Swin + Focal Loss models
â”‚   â””â”€â”€ swin_dual_branch/            # Dual-branch models â­
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ swin_ablation_20251009_214816/  # ViT experiments
â”‚   â””â”€â”€ swin_ablation_20251009_222821/  # Swin experiments
â”œâ”€â”€ picture/
â”‚   â”œâ”€â”€ generated/                   # Visualization charts
â”‚   â””â”€â”€ processed/                   # Sample analysis
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README_EN.md                 # English documentation
â”‚   â”œâ”€â”€ COMPARISON_WITH_SOTA.md      # SOTA comparison
â”‚   â””â”€â”€ COMPLETE_TUTORIAL.md         # Complete tutorial
â””â”€â”€ README.md                        # Main README (Chinese)
```

---

## ğŸ“ˆ Performance Analysis

### Class-wise Performance

**BCN20000 Dataset (Swin Dual-Branch)**:
- **Overall Accuracy**: 93.33%
- **Macro F1**: 0.930
- **Weighted F1**: 0.936
- **Melanoma F1**: 0.974 â­

**HAM10000 Dataset (Swin Dual-Branch)**:
- **Overall Accuracy**: 98.90% ğŸ†
- **Macro F1**: 0.984
- **Weighted F1**: 0.989
- **Melanoma F1**: 0.977 â­

**Key Observations**:
- âœ… Near-perfect classification on HAM10000 (98.90%)
- âœ… Exceptional melanoma detection (F1: 0.974-0.977)
- âœ… Balanced performance across all classes (Macro F1: 0.930-0.984)
- âœ… Significant improvement over baseline models

---

## ğŸ“š References

### Core Papers

1. **Swin Transformer**
   - Liu, Z., et al. (2021). "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows." *ICCV 2021*.
   - [arXiv:2103.14030](https://arxiv.org/abs/2103.14030)

2. **Focal Loss**
   - Lin, T. Y., et al. (2017). "Focal Loss for Dense Object Detection." *ICCV 2017*.
   - [arXiv:1708.02002](https://arxiv.org/abs/1708.02002)

3. **Vision Transformer**
   - Dosovitskiy, A., et al. (2020). "An Image is Worth 16x16 Words." *ICLR 2021*.
   - [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)

### Datasets

- **BCN20000**: Barcelona Hospital Clinic dataset
- **HAM10000**: Human Against Machine with 10000 training images
  - Tschandl, P., et al. (2018). "The HAM10000 dataset." *Scientific Data*.

---

## ğŸ“§ Contact

- **Author**: HeroZyy
- **Email**: a1048666899@gmail.com
- **GitHub**: [@HeroZyy](https://github.com/HeroZyy)
- **Project Link**: [https://github.com/HeroZyy/skin-lesion-classification](https://github.com/HeroZyy/skin-lesion-classification)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ“Š Citation

If you use this code in your research, please cite:

```bibtex
@misc{skin_lesion_swin_2025,
  title={Swin Transformer with Focal Loss for Skin Lesion Classification},
  author={HeroZyy},
  year={2025},
  publisher={GitHub},
  howpublished={\url{https://github.com/HeroZyy/skin-lesion-classification}}
}
```

---

<div align="center">

**Made with â¤ï¸ for advancing medical AI**

[â¬† Back to Top](#-swin-transformer--focal-loss-for-skin-lesion-classification)

</div>


