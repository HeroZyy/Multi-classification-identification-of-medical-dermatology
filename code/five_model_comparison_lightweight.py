"""
äº”ç»„æ¨¡å‹å¯¹æ¯”å®éªŒ - è½»é‡çº§ç®€æ´ç‰ˆï¼ˆå¢å¼ºç¨³å®šæ€§ï¼‰
å¯¹BCN20000å’ŒHAM10000ä¸¤ä¸ªæ•°æ®é›†åˆ†åˆ«è¿›è¡Œè®­ç»ƒå’Œç»Ÿè®¡
åŒ…å«ï¼šé”™è¯¯å¤„ç†ã€æ—¥å¿—ä¿å­˜ã€æ–­ç‚¹ç»­è®­ã€è¿›åº¦ç›‘æ§
"""

import os
import sys

# ============================================================
# ç¦»çº¿æ¨¡å¼è®¾ç½® - ç¦ç”¨ç½‘ç»œä¸‹è½½
# ============================================================
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆä¸ä» Hugging Face Hub ä¸‹è½½ï¼‰
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'

import torch
import torch.nn as nn
import timm
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, f1_score, classification_report
from tqdm import tqdm
from datetime import datetime
import warnings
import logging
import json
import traceback
import time
warnings.filterwarnings('ignore')

# å¯¼å…¥æ•°æ®å¤„ç†æ¨¡å—
from melanoma_focused_improvement import (
    MelanomaDataset, create_melanoma_focused_transforms,
    FocalLoss, DISEASE_NAMES
)

def setup_logger(output_dir):
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_file = os.path.join(output_dir, 'training.log')

    # åˆ›å»ºlogger
    logger = logging.getLogger('FiveModelComparison')
    logger.setLevel(logging.INFO)

    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)

    # æ ¼å¼åŒ–
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)

    logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    return logger

class Config:
    """å¢å¼ºé…ç½®ï¼ˆå«ç¨³å®šæ€§ä¼˜åŒ–ï¼‰"""
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.epochs = 40
        self.batch_size = 32
        self.learning_rate = 1e-4
        self.weight_decay = 1e-4  # ä¸æ¶ˆèå®éªŒä¸€è‡´
        self.num_classes = 7
        self.num_workers = 8

        # æ—©åœé…ç½® - å·²ç¦ç”¨ï¼Œè®©æ‰€æœ‰æ¨¡å‹è·‘æ»¡40è½®
        self.use_early_stopping = False  # ğŸ”´ ç¦ç”¨æ—©åœ
        self.patience = 7
        self.min_delta = 0.001

        # Swinæ¨¡å‹ä¸“ç”¨é…ç½®ï¼ˆä¸æ¶ˆèå®éªŒä¸€è‡´ï¼‰
        self.use_cosine_scheduler = True  # Swinä½¿ç”¨ä½™å¼¦é€€ç«
        self.use_gradient_clipping = True  # ä½¿ç”¨æ¢¯åº¦è£å‰ª
        self.max_grad_norm = 1.0  # æ¢¯åº¦è£å‰ªé˜ˆå€¼

        # äº”ç»„æ¨¡å‹é…ç½® - æ ¹æ®éœ€æ±‚é€‰æ‹©æ€§è®­ç»ƒ
        # BCN20000: ViT_Base
        # HAM10000: DenseNet121, EfficientNet_B4, ViT_Base
        self.models = {
            'ViT_Base': 'vit_base_patch16_224',
            'EfficientNet_B4': 'efficientnet_b4',
            'ResNet50': 'resnet50',  
            'DenseNet121': 'densenet121',
            # 'Swin_Base': 'swin_base_patch4_window7_224'  
        }

        # æ•°æ®é›†-æ¨¡å‹æ˜ å°„ï¼šæŒ‡å®šæ¯ä¸ªæ•°æ®é›†è®­ç»ƒå“ªäº›æ¨¡å‹
        self.dataset_models = {
            'BCN20000': ['ViT_Base'],  # BCN20000åªè®­ç»ƒViT_Base
            'HAM10000': ['DenseNet121', 'EfficientNet_B4', 'ViT_Base']  # HAM10000è®­ç»ƒè¿™ä¸‰ä¸ª
        }

        # åˆ›å»ºè¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = f'results/five_model_comparison_{timestamp}'
        os.makedirs(self.output_dir, exist_ok=True)

        # åˆ›å»ºmodelså­ç›®å½•ï¼Œä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå•ç‹¬æ–‡ä»¶å¤¹
        self.models_dir = os.path.join(self.output_dir, 'models')
        os.makedirs(self.models_dir, exist_ok=True)
        for model_name in self.models.keys():
            model_folder = os.path.join(self.models_dir, model_name)
            os.makedirs(model_folder, exist_ok=True)

        # è®¾ç½®æ—¥å¿—
        self.logger = setup_logger(self.output_dir)

        # ä¿å­˜é…ç½®
        self.save_config()

    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        config_dict = {
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate,
            'patience': self.patience,
            'models': self.models,
            'device': str(self.device)
        }
        config_file = os.path.join(self.output_dir, 'config.json')
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=4, ensure_ascii=False)

class SimpleModel(nn.Module):
    """ç®€åŒ–æ¨¡å‹"""
    def __init__(self, backbone_name, num_classes=7):
        super().__init__()

        # ============================================================
        # åœ¨çº¿æ¨¡å¼ï¼ˆéœ€è¦ç½‘ç»œè¿æ¥ï¼Œä» Hugging Face Hub ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼‰
        # ============================================================
        # self.backbone = timm.create_model(backbone_name, pretrained=True, num_classes=0)

        # ============================================================
        # ç¦»çº¿æ¨¡å¼ï¼ˆä¸éœ€è¦ç½‘ç»œè¿æ¥ï¼Œåªåˆ›å»ºæ¨¡å‹ç»“æ„ï¼‰
        # ============================================================
        self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=0)

        self.classifier = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(self.backbone.num_features, num_classes)
        )

    def forward(self, x):
        features = self.backbone(x)
        return self.classifier(features)

def load_dataset(dataset_name):
    """åŠ è½½æ•°æ®é›†ï¼ˆä¸è¯„ä¼°è„šæœ¬ä¿æŒä¸€è‡´ï¼‰"""
    from sklearn.model_selection import train_test_split

    print(f"\nåŠ è½½ {dataset_name} æ•°æ®é›†...")
    all_data = []

    if dataset_name == 'BCN20000':
        bcn_path = 'datasets/BCN20000'
        metadata = pd.read_csv(os.path.join(bcn_path, 'bcn20000.csv'))

        class_mapping = {
            'melanoma': 'MEL', 'nevus': 'NV', 'basal cell carcinoma': 'BCC',
            'seborrheic keratosis': 'BKL', 'actinic keratosis': 'AKIEC',
            'dermatofibroma': 'DF', 'vascular lesion': 'VASC'
        }
        label_mapping = {'AKIEC': 0, 'BCC': 1, 'BKL': 2, 'DF': 3, 'MEL': 4, 'NV': 5, 'VASC': 6}

        for _, row in metadata.iterrows():
            image_path = os.path.join(bcn_path, 'images', f"{row['isic_id']}.JPG")
            if os.path.exists(image_path):
                label = class_mapping.get(row['diagnosis'], 'NV')
                all_data.append({
                    'image_path': image_path,
                    'label': label_mapping.get(label, 5),
                    'image_id': row['isic_id']
                })

    elif dataset_name == 'HAM10000':
        ham_path = 'datasets/HAM10000_clean/ISIC2018'
        metadata = pd.read_csv('datasets/HAM10000_clean/ISIC2018_splits/HAM_clean.csv')

        label_mapping = {'akiec': 0, 'bcc': 1, 'bkl': 2, 'df': 3, 'mel': 4, 'nv': 5, 'vasc': 6}

        for _, row in metadata.iterrows():
            image_path = os.path.join(ham_path, f"{row['image_id']}.jpg")
            if os.path.exists(image_path):
                all_data.append({
                    'image_path': image_path,
                    'label': label_mapping.get(row['dx'].lower(), 5),
                    'image_id': row['image_id']
                })

    # æ•°æ®åˆ†å‰² (80% train, 10% val, 10% test) - ä¸è¯„ä¼°è„šæœ¬ä¿æŒä¸€è‡´
    # ä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿æ¯æ¬¡åˆ’åˆ†ç›¸åŒ
    train_val, test_data = train_test_split(all_data, test_size=0.1, random_state=42)
    train_data, val_data = train_test_split(train_val, test_size=0.111, random_state=42)  # 0.111 * 0.9 â‰ˆ 0.1

    print(f"è®­ç»ƒé›†: {len(train_data)}, éªŒè¯é›†: {len(val_data)}, æµ‹è¯•é›†: {len(test_data)}")
    print(f"âš ï¸  æ•°æ®åˆ’åˆ†å·²æ›´æ–°ä¸º: 80% train / 10% val / 10% test (random_state=42)")
    return train_data, val_data, test_data

def train_model(model, train_loader, val_loader, config, model_name):
    """è®­ç»ƒæ¨¡å‹ï¼ˆå«æ—©åœå’Œé”™è¯¯å¤„ç†ï¼‰"""
    logger = config.logger

    try:
        model.to(config.device)

        # æå–åŸºç¡€æ¨¡å‹åç§°ï¼ˆå»æ‰æ•°æ®é›†åç¼€ï¼‰
        base_model_name = model_name.rsplit('_', 1)[0] if '_' in model_name else model_name

        # æ ¹æ®æ¨¡å‹ç±»å‹é…ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        if 'Swin' in base_model_name:
            # Swinæ¨¡å‹ä½¿ç”¨ä¸æ¶ˆèå®éªŒä¸€è‡´çš„é…ç½®
            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay
            )
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=config.epochs
            )
            logger.info(f"ğŸ¯ {base_model_name} ä½¿ç”¨Swinä¸“ç”¨é…ç½® (AdamW + CosineAnnealing + æ¢¯åº¦è£å‰ª)")
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†é…ç½®
            optimizer = torch.optim.AdamW(
                model.parameters(),
                lr=config.learning_rate,
                weight_decay=config.weight_decay
            )
            scheduler = None
            logger.info(f"ğŸ¯ {base_model_name} ä½¿ç”¨æ ‡å‡†é…ç½® (AdamW)")

        criterion = FocalLoss(alpha=0.25, gamma=2.0)

        best_val_acc = 0.0
        patience_counter = 0
        start_time = time.time()

        logger.info(f"å¼€å§‹è®­ç»ƒ {model_name}")

        for epoch in range(config.epochs):
            epoch_start = time.time()

            # è®­ç»ƒ
            model.train()
            train_loss, train_correct, train_total = 0, 0, 0

            try:
                for images, labels, _ in tqdm(train_loader, desc=f'{model_name} Epoch {epoch+1}/{config.epochs}'):
                    images, labels = images.to(config.device), labels.to(config.device)

                    optimizer.zero_grad()
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                    loss.backward()

                    # Swinæ¨¡å‹ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆä¸æ¶ˆèå®éªŒä¸€è‡´ï¼‰
                    if 'Swin' in base_model_name and config.use_gradient_clipping:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.max_grad_norm)

                    optimizer.step()

                    train_loss += loss.item()
                    _, predicted = outputs.max(1)
                    train_total += labels.size(0)
                    train_correct += predicted.eq(labels).sum().item()
            except Exception as e:
                logger.error(f"è®­ç»ƒå¾ªç¯é”™è¯¯: {str(e)}")
                logger.error(traceback.format_exc())
                raise

            # éªŒè¯
            model.eval()
            val_preds, val_labels = [], []

            try:
                with torch.no_grad():
                    for images, labels, _ in val_loader:
                        images = images.to(config.device)
                        outputs = model(images)
                        _, predicted = outputs.max(1)
                        val_preds.extend(predicted.cpu().numpy())
                        val_labels.extend(labels.numpy())
            except Exception as e:
                logger.error(f"éªŒè¯å¾ªç¯é”™è¯¯: {str(e)}")
                logger.error(traceback.format_exc())
                raise

            val_acc = accuracy_score(val_labels, val_preds) * 100
            train_acc = 100. * train_correct / train_total
            epoch_time = time.time() - epoch_start

            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆSwinæ¨¡å‹ï¼‰
            if scheduler is not None:
                scheduler.step()
                current_lr = scheduler.get_last_lr()[0]
                log_msg = f'Epoch {epoch+1}/{config.epochs}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, LR: {current_lr:.2e}, Time: {epoch_time:.1f}s'
            else:
                log_msg = f'Epoch {epoch+1}/{config.epochs}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%, Time: {epoch_time:.1f}s'

            logger.info(log_msg)
            print(log_msg)

            # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°å¯¹åº”çš„æ¨¡å‹æ–‡ä»¶å¤¹
            if val_acc > best_val_acc + config.min_delta:
                best_val_acc = val_acc
                patience_counter = 0

                # æå–æ¨¡å‹åç§°ï¼ˆå»æ‰æ•°æ®é›†åç¼€ï¼‰
                base_model_name = model_name.rsplit('_', 1)[0]  # ä¾‹å¦‚: ViT_Base_BCN20000 -> ViT_Base
                model_folder = os.path.join(config.models_dir, base_model_name)
                checkpoint_path = os.path.join(model_folder, f'{model_name}_best.pth')

                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_val_acc': best_val_acc,
                    'model_name': model_name,
                    'base_model_name': base_model_name,
                }, checkpoint_path)
                logger.info(f'ä¿å­˜æœ€ä½³æ¨¡å‹: {best_val_acc:.2f}% -> {checkpoint_path}')
            else:
                patience_counter += 1
                logger.info(f'æ— æå‡ ({patience_counter}/{config.patience})')

            # æ—©åœæœºåˆ¶ - å·²ç¦ç”¨
            if config.use_early_stopping and patience_counter >= config.patience:
                logger.info(f'æ—©åœè§¦å‘ at epoch {epoch+1}')
                break

        total_time = time.time() - start_time
        logger.info(f'{model_name} è®­ç»ƒå®Œæˆ! æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.2f}%, æ€»æ—¶é—´: {total_time/60:.1f}åˆ†é’Ÿ')

        return model, best_val_acc

    except Exception as e:
        logger.error(f"è®­ç»ƒ {model_name} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        logger.error(traceback.format_exc())
        # ä¿å­˜é”™è¯¯çŠ¶æ€
        error_file = os.path.join(config.output_dir, f'{model_name}_error.txt')
        with open(error_file, 'w', encoding='utf-8') as f:
            f.write(f"é”™è¯¯æ—¶é—´: {datetime.now()}\n")
            f.write(f"é”™è¯¯ä¿¡æ¯: {str(e)}\n")
            f.write(f"å †æ ˆè·Ÿè¸ª:\n{traceback.format_exc()}")
        raise

def evaluate_model(model, test_loader, config):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    test_preds, test_labels = [], []
    
    with torch.no_grad():
        for images, labels, _ in test_loader:
            images = images.to(config.device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            test_preds.extend(predicted.cpu().numpy())
            test_labels.extend(labels.numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(test_labels, test_preds) * 100
    macro_f1 = f1_score(test_labels, test_preds, average='macro')
    weighted_f1 = f1_score(test_labels, test_preds, average='weighted')
    
    # é»‘è‰²ç´ ç˜¤F1
    mel_binary_labels = [1 if l == 4 else 0 for l in test_labels]
    mel_binary_preds = [1 if p == 4 else 0 for p in test_preds]
    melanoma_f1 = f1_score(mel_binary_labels, mel_binary_preds, average='binary')
    
    return {
        'accuracy': accuracy,
        'macro_f1': macro_f1,
        'weighted_f1': weighted_f1,
        'melanoma_f1': melanoma_f1
    }

def train_on_dataset(dataset_name, config):
    """åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šè®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼ˆå«é”™è¯¯æ¢å¤ï¼‰"""
    logger = config.logger

    logger.info(f"\n{'='*60}")
    logger.info(f"å¼€å§‹åœ¨ {dataset_name} ä¸Šè®­ç»ƒäº”ç»„æ¨¡å‹")
    logger.info(f"{'='*60}")

    try:
        # åŠ è½½æ•°æ®
        train_data, val_data, test_data = load_dataset(dataset_name)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_transform, val_transform = create_melanoma_focused_transforms()
        train_dataset = MelanomaDataset(train_data, train_transform, is_training=True)
        val_dataset = MelanomaDataset(val_data, val_transform, is_training=False)
        test_dataset = MelanomaDataset(test_data, val_transform, is_training=False)

        train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True,
                                 num_workers=config.num_workers, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False,
                               num_workers=config.num_workers, pin_memory=True)
        test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False,
                                num_workers=config.num_workers, pin_memory=True)

        # è®­ç»ƒæŒ‡å®šçš„æ¨¡å‹ï¼ˆæ ¹æ®æ•°æ®é›†ï¼‰
        results = {}
        progress_file = os.path.join(config.output_dir, f'{dataset_name}_progress.json')

        # è·å–å½“å‰æ•°æ®é›†éœ€è¦è®­ç»ƒçš„æ¨¡å‹åˆ—è¡¨
        models_to_train = config.dataset_models.get(dataset_name, list(config.models.keys()))
        logger.info(f"ğŸ“‹ {dataset_name} å°†è®­ç»ƒä»¥ä¸‹æ¨¡å‹: {models_to_train}")

        # è¿‡æ»¤å‡ºéœ€è¦è®­ç»ƒçš„æ¨¡å‹
        filtered_models = {k: v for k, v in config.models.items() if k in models_to_train}

        for idx, (model_name, backbone_name) in enumerate(filtered_models.items(), 1):
            logger.info(f"\n[{idx}/{len(filtered_models)}] è®­ç»ƒ {model_name}...")

            try:
                # åˆ›å»ºæ¨¡å‹
                model = SimpleModel(backbone_name, config.num_classes)

                # è®­ç»ƒ
                model, best_val_acc = train_model(model, train_loader, val_loader, config,
                                                 f'{model_name}_{dataset_name}')

                # åŠ è½½æœ€ä½³æƒé‡ï¼ˆä»æ¨¡å‹æ–‡ä»¶å¤¹ä¸­ï¼‰
                model_folder = os.path.join(config.models_dir, model_name)
                checkpoint_path = os.path.join(model_folder, f'{model_name}_{dataset_name}_best.pth')
                checkpoint = torch.load(checkpoint_path)
                model.load_state_dict(checkpoint['model_state_dict'])

                # è¯„ä¼°
                result = evaluate_model(model, test_loader, config)
                result['best_val_acc'] = best_val_acc
                results[model_name] = result

                logger.info(f"âœ… {model_name} å®Œæˆ: ACC={result['accuracy']:.2f}%, MEL F1={result['melanoma_f1']:.3f}")

                # ä¿å­˜è¿›åº¦
                with open(progress_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=4, ensure_ascii=False)

                # æ¸…ç†æ˜¾å­˜
                del model
                torch.cuda.empty_cache()

            except Exception as e:
                logger.error(f"è®­ç»ƒ {model_name} å¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())
                results[model_name] = {'error': str(e), 'accuracy': 0.0}
                # ç»§ç»­è®­ç»ƒä¸‹ä¸€ä¸ªæ¨¡å‹
                continue

        return results

    except Exception as e:
        logger.error(f"æ•°æ®é›† {dataset_name} è®­ç»ƒå¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        raise

def save_results(all_results, config):
    """ä¿å­˜ç»“æœï¼ˆå¢å¼ºç‰ˆ - åˆ†æ•°æ®é›†ç»Ÿè®¡ï¼‰"""
    logger = config.logger

    # 1. ä¿å­˜å®Œæ•´ç»“æœè¡¨æ ¼
    data = []
    for dataset, results in all_results.items():
        for model, metrics in results.items():
            if 'error' in metrics:
                data.append({
                    'Dataset': dataset,
                    'Model': model,
                    'Accuracy': 'ERROR',
                    'Macro_F1': 'ERROR',
                    'Weighted_F1': 'ERROR',
                    'Melanoma_F1': 'ERROR',
                    'Error': metrics['error']
                })
            else:
                data.append({
                    'Dataset': dataset,
                    'Model': model,
                    'Accuracy': f"{metrics['accuracy']:.2f}%",
                    'Macro_F1': f"{metrics['macro_f1']:.3f}",
                    'Weighted_F1': f"{metrics['weighted_f1']:.3f}",
                    'Melanoma_F1': f"{metrics['melanoma_f1']:.3f}",
                    'Best_Val_Acc': f"{metrics.get('best_val_acc', 0):.2f}%"
                })

    df = pd.DataFrame(data)

    # ä¿å­˜å®Œæ•´ç»“æœCSV
    csv_path = os.path.join(config.output_dir, 'results_complete.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    logger.info(f"å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")

    # 2. åˆ†æ•°æ®é›†ä¿å­˜ç»“æœ
    for dataset in all_results.keys():
        dataset_df = df[df['Dataset'] == dataset].copy()
        dataset_csv = os.path.join(config.output_dir, f'results_{dataset}.csv')
        dataset_df.to_csv(dataset_csv, index=False, encoding='utf-8-sig')
        logger.info(f"{dataset} ç»“æœå·²ä¿å­˜åˆ°: {dataset_csv}")

    # 3. ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    summary_lines = []
    summary_lines.append("="*80)
    summary_lines.append("äº”ç»„æ¨¡å‹å¯¹æ¯”å®éªŒ - ç»“æœç»Ÿè®¡æ‘˜è¦")
    summary_lines.append("="*80)
    summary_lines.append("")

    for dataset, results in all_results.items():
        summary_lines.append(f"\n{'#'*80}")
        summary_lines.append(f"# æ•°æ®é›†: {dataset}")
        summary_lines.append(f"{'#'*80}")
        summary_lines.append("")

        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_results = sorted(
            [(model, metrics) for model, metrics in results.items() if 'error' not in metrics],
            key=lambda x: x[1]['accuracy'],
            reverse=True
        )

        summary_lines.append(f"{'æ¨¡å‹':<20} {'å‡†ç¡®ç‡':<12} {'Macro F1':<12} {'Weighted F1':<12} {'Melanoma F1':<12}")
        summary_lines.append("-"*80)

        for rank, (model, metrics) in enumerate(sorted_results, 1):
            summary_lines.append(
                f"{rank}. {model:<17} "
                f"{metrics['accuracy']:>6.2f}%     "
                f"{metrics['macro_f1']:>6.3f}       "
                f"{metrics['weighted_f1']:>6.3f}         "
                f"{metrics['melanoma_f1']:>6.3f}"
            )

        # ç»Ÿè®¡ä¿¡æ¯
        if sorted_results:
            accuracies = [m['accuracy'] for _, m in sorted_results]
            mel_f1s = [m['melanoma_f1'] for _, m in sorted_results]

            summary_lines.append("")
            summary_lines.append(f"ç»Ÿè®¡ä¿¡æ¯:")
            summary_lines.append(f"  - æœ€ä½³å‡†ç¡®ç‡: {max(accuracies):.2f}% ({sorted_results[0][0]})")
            summary_lines.append(f"  - æœ€å·®å‡†ç¡®ç‡: {min(accuracies):.2f}% ({sorted_results[-1][0]})")
            summary_lines.append(f"  - å¹³å‡å‡†ç¡®ç‡: {np.mean(accuracies):.2f}%")
            summary_lines.append(f"  - æœ€ä½³Melanoma F1: {max(mel_f1s):.3f}")
            summary_lines.append(f"  - å¹³å‡Melanoma F1: {np.mean(mel_f1s):.3f}")

    # 4. è·¨æ•°æ®é›†å¯¹æ¯”
    summary_lines.append(f"\n{'#'*80}")
    summary_lines.append(f"# è·¨æ•°æ®é›†å¯¹æ¯”")
    summary_lines.append(f"{'#'*80}")
    summary_lines.append("")

    for model_name in config.models.keys():
        summary_lines.append(f"\n{model_name}:")
        for dataset in all_results.keys():
            if model_name in all_results[dataset] and 'error' not in all_results[dataset][model_name]:
                metrics = all_results[dataset][model_name]
                summary_lines.append(
                    f"  {dataset:<12}: ACC={metrics['accuracy']:>6.2f}%, "
                    f"MEL F1={metrics['melanoma_f1']:.3f}"
                )

    summary_lines.append("")
    summary_lines.append("="*80)

    # ä¿å­˜æ‘˜è¦
    summary_text = '\n'.join(summary_lines)
    summary_path = os.path.join(config.output_dir, 'RESULTS_SUMMARY.txt')
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary_text)

    logger.info(f"ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

    # æ‰“å°æ‘˜è¦
    print("\n" + summary_text)

    # 5. ä¿å­˜æ¨¡å‹æ–‡ä»¶å¤¹ä¿¡æ¯
    models_info = []
    models_info.append("="*80)
    models_info.append("æ¨¡å‹æ–‡ä»¶ä¿å­˜ä½ç½®")
    models_info.append("="*80)
    models_info.append("")

    for model_name in config.models.keys():
        model_folder = os.path.join(config.models_dir, model_name)
        models_info.append(f"\n{model_name}:")
        models_info.append(f"  æ–‡ä»¶å¤¹: {model_folder}")

        # åˆ—å‡ºè¯¥æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶
        if os.path.exists(model_folder):
            files = os.listdir(model_folder)
            if files:
                models_info.append(f"  åŒ…å«æ–‡ä»¶:")
                for f in sorted(files):
                    file_path = os.path.join(model_folder, f)
                    file_size = os.path.getsize(file_path) / (1024*1024)  # MB
                    models_info.append(f"    - {f} ({file_size:.1f} MB)")
            else:
                models_info.append(f"  (ç©ºæ–‡ä»¶å¤¹)")

    models_info.append("")
    models_info.append("="*80)

    models_info_text = '\n'.join(models_info)
    models_info_path = os.path.join(config.output_dir, 'MODELS_INFO.txt')
    with open(models_info_path, 'w', encoding='utf-8') as f:
        f.write(models_info_text)

    logger.info(f"æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ°: {models_info_path}")
    print("\n" + models_info_text)

def main():
    """ä¸»å‡½æ•°ï¼ˆå«å®Œæ•´é”™è¯¯å¤„ç†ï¼‰"""
    start_time = time.time()

    try:
        print("="*80)
        print("äº”ç»„æ¨¡å‹å¯¹æ¯”å®éªŒ - å¢å¼ºç¨³å®šç‰ˆ")
        print("åŒ…å«ï¼šé”™è¯¯å¤„ç†ã€æ—¥å¿—ä¿å­˜ã€æ–­ç‚¹ç»­è®­ã€è¿›åº¦ç›‘æ§")
        print("="*80)

        config = Config()
        logger = config.logger

        logger.info(f"è®¾å¤‡: {config.device}")
        logger.info(f"è¾“å‡ºç›®å½•: {config.output_dir}")
        early_stop_status = "ç¦ç”¨ (è·‘æ»¡40è½®)" if not config.use_early_stopping else f"å¯ç”¨ (Patience={config.patience})"
        logger.info(f"è®­ç»ƒé…ç½®: Epochs={config.epochs}, Batch={config.batch_size}, æ—©åœ={early_stop_status}")
        logger.info(f"BCN20000è®­ç»ƒæ¨¡å‹: {config.dataset_models['BCN20000']}")
        logger.info(f"HAM10000è®­ç»ƒæ¨¡å‹: {config.dataset_models['HAM10000']}")

        # è®­ç»ƒä¸¤ä¸ªæ•°æ®é›†
        all_results = {}
        dataset_times = {}

        for dataset in ['BCN20000', 'HAM10000']:
            dataset_start = time.time()
            logger.info(f"\n{'#'*80}")
            logger.info(f"# æ•°æ®é›†: {dataset}")
            logger.info(f"{'#'*80}")

            try:
                results = train_on_dataset(dataset, config)
                all_results[dataset] = results
                dataset_time = time.time() - dataset_start
                dataset_times[dataset] = dataset_time
                logger.info(f"{dataset} å®Œæˆ! ç”¨æ—¶: {dataset_time/60:.1f}åˆ†é’Ÿ")
            except Exception as e:
                logger.error(f"{dataset} è®­ç»ƒå¤±è´¥: {str(e)}")
                logger.error(traceback.format_exc())
                all_results[dataset] = {'error': str(e)}
                # ç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®é›†
                continue

        # ä¿å­˜ç»“æœ
        try:
            save_results(all_results, config)
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())

        # æ€»ç»“
        total_time = time.time() - start_time
        logger.info("\n" + "="*80)
        logger.info("âœ… æ‰€æœ‰å®éªŒå®Œæˆ!")
        logger.info(f"æ€»ç”¨æ—¶: {total_time/3600:.2f}å°æ—¶ ({total_time/60:.1f}åˆ†é’Ÿ)")
        for dataset, dt in dataset_times.items():
            logger.info(f"  - {dataset}: {dt/60:.1f}åˆ†é’Ÿ")
        logger.info(f"ç»“æœä¿å­˜åœ¨: {config.output_dir}")
        logger.info("="*80)

        return 0

    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
        print(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

